{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installations","metadata":{}},{"cell_type":"code","source":"!pip install ftfy regex tqdm\n!pip install git+https://github.com/openai/CLIP.git\n!pip install torchinfo","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:24:49.12532Z","iopub.execute_input":"2022-10-04T02:24:49.12661Z","iopub.status.idle":"2022-10-04T02:25:23.834055Z","shell.execute_reply.started":"2022-10-04T02:24:49.126494Z","shell.execute_reply":"2022-10-04T02:25:23.832826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport os\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport clip\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchinfo import summary\nfrom tqdm import tqdm\nfrom torch.nn.functional import normalize\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nimport math\nimport cv2\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:25:23.840454Z","iopub.execute_input":"2022-10-04T02:25:23.84293Z","iopub.status.idle":"2022-10-04T02:25:26.787502Z","shell.execute_reply.started":"2022-10-04T02:25:23.842888Z","shell.execute_reply":"2022-10-04T02:25:26.786485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CLIP model","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:25:26.788985Z","iopub.execute_input":"2022-10-04T02:25:26.789628Z","iopub.status.idle":"2022-10-04T02:25:26.797225Z","shell.execute_reply.started":"2022-10-04T02:25:26.789591Z","shell.execute_reply":"2022-10-04T02:25:26.794953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip.available_models()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:25:26.799545Z","iopub.execute_input":"2022-10-04T02:25:26.800357Z","iopub.status.idle":"2022-10-04T02:25:26.828554Z","shell.execute_reply.started":"2022-10-04T02:25:26.800315Z","shell.execute_reply":"2022-10-04T02:25:26.827306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMB_SIZE = 64\nOUTPUT_CLASSES = 1424  #1424   #1034 #1902 #1346\nBATCH_SIZE = 64\nEPOCH = 1\nWORKERS = 4\nVAL_LOSS = []\nTRAIN_LOSS = []\nMARGIN = 0.5 # TRY # 0 for faster convergence, larger may be beneficial\n# init_lr = 3e-4","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:25:29.775349Z","iopub.execute_input":"2022-10-04T02:25:29.775718Z","iopub.status.idle":"2022-10-04T02:25:29.781492Z","shell.execute_reply.started":"2022-10-04T02:25:29.775686Z","shell.execute_reply":"2022-10-04T02:25:29.779918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model, preprocess = clip.load(\"ViT-L/14\", device=device, jit=True) ","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:25:31.159101Z","iopub.execute_input":"2022-10-04T02:25:31.160076Z","iopub.status.idle":"2022-10-04T02:25:31.165137Z","shell.execute_reply.started":"2022-10-04T02:25:31.160021Z","shell.execute_reply":"2022-10-04T02:25:31.163881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip_code = \"ViT-L/14@336px\"\nmodel, preprocess = clip.load(\"ViT-L/14@336px\", jit=True, device=device)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:25:31.455663Z","iopub.execute_input":"2022-10-04T02:25:31.456033Z","iopub.status.idle":"2022-10-04T02:25:49.549756Z","shell.execute_reply.started":"2022-10-04T02:25:31.456Z","shell.execute_reply":"2022-10-04T02:25:49.548757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Laion5B","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nroot_path = \"/kaggle/input/ab-balanced-laion5b/dataset\"\n\nfor root, dirnames, filenames in os.walk(root_path):\n    dir_names = dirnames\n    break","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:25:49.552072Z","iopub.execute_input":"2022-10-04T02:25:49.552449Z","iopub.status.idle":"2022-10-04T02:25:49.567474Z","shell.execute_reply.started":"2022-10-04T02:25:49.552413Z","shell.execute_reply":"2022-10-04T02:25:49.566525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\n\nimg_path = []\ncaption = []\nfor dir in dir_names:\n    for f_name in glob(os.path.join(root_path, dir, '*.json')):\n        df_json = pd.read_json(os.path.join(root_path, dir,f_name), typ='series')\n        if df_json['status'] == \"success\":\n            img_path.append(os.path.join(root_path, dir,f_name.split(\".\")[0] + '.jpg'))\n            caption.append(df_json['caption'])\n        \ndf_laion5b = pd.DataFrame({'img_path': img_path, 'class': caption})","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:25:49.568637Z","iopub.execute_input":"2022-10-04T02:25:49.568946Z","iopub.status.idle":"2022-10-04T02:30:43.961643Z","shell.execute_reply.started":"2022-10-04T02:25:49.56891Z","shell.execute_reply":"2022-10-04T02:30:43.960603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_laion5b)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:30:43.964552Z","iopub.execute_input":"2022-10-04T02:30:43.964938Z","iopub.status.idle":"2022-10-04T02:30:43.973835Z","shell.execute_reply.started":"2022-10-04T02:30:43.9649Z","shell.execute_reply":"2022-10-04T02:30:43.972919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_laion5b.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:30:43.975292Z","iopub.execute_input":"2022-10-04T02:30:43.975601Z","iopub.status.idle":"2022-10-04T02:30:43.992553Z","shell.execute_reply.started":"2022-10-04T02:30:43.975574Z","shell.execute_reply":"2022-10-04T02:30:43.991402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_laion5b['img_path'][0]","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:30:43.993966Z","iopub.execute_input":"2022-10-04T02:30:43.994313Z","iopub.status.idle":"2022-10-04T02:30:43.998613Z","shell.execute_reply.started":"2022-10-04T02:30:43.994279Z","shell.execute_reply":"2022-10-04T02:30:43.997396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_laion5b['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:30:44.000157Z","iopub.execute_input":"2022-10-04T02:30:44.000536Z","iopub.status.idle":"2022-10-04T02:30:44.01879Z","shell.execute_reply.started":"2022-10-04T02:30:44.00047Z","shell.execute_reply":"2022-10-04T02:30:44.017894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WIT","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom glob import glob","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:30:44.020203Z","iopub.execute_input":"2022-10-04T02:30:44.020621Z","iopub.status.idle":"2022-10-04T02:30:44.028683Z","shell.execute_reply.started":"2022-10-04T02:30:44.020585Z","shell.execute_reply":"2022-10-04T02:30:44.027896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_path = \"../input/ab-wit-ds\"\n\nfor root, dirnames, filenames in os.walk(root_path):\n    dir_names = dirnames\n    break","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:30:44.031807Z","iopub.execute_input":"2022-10-04T02:30:44.032093Z","iopub.status.idle":"2022-10-04T02:30:44.047101Z","shell.execute_reply.started":"2022-10-04T02:30:44.03207Z","shell.execute_reply":"2022-10-04T02:30:44.046219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_path = []\ncaption = []\nfor dir in dir_names:\n    for f_name in glob(os.path.join(root_path, dir, '00000', '*.json')):\n        df_json = pd.read_json(f_name, typ='series') #os.path.join(root_path, dir, '00000', f_name)\n        if df_json['status'] == \"success\":\n            img_path.append(f_name.strip().split(\".\")[2] + '.jpg')\n            caption.append(df_json['caption'])\n        \ndf_wit_1 = pd.DataFrame({'img_path': img_path, 'class': caption})","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:30:44.050973Z","iopub.execute_input":"2022-10-04T02:30:44.05167Z","iopub.status.idle":"2022-10-04T02:32:46.268605Z","shell.execute_reply.started":"2022-10-04T02:30:44.051636Z","shell.execute_reply":"2022-10-04T02:32:46.267457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# root_path = \"../input/ab-wit-ds-2\"\n\n# for root, dirnames, filenames in os.walk(root_path):\n#     dir_names = dirnames\n#     break\n    \n# img_path = []\n# caption = []\n# for dir in dir_names:\n#     for f_name in glob(os.path.join(root_path, dir, '00000', '*.json')):\n#         df_json = pd.read_json(f_name, typ='series') #os.path.join(root_path, dir, '00000', f_name)\n#         if df_json['status'] == \"success\":\n#             img_path.append(f_name.strip().split(\".\")[2] + '.jpg')\n#             caption.append(df_json['caption'])\n        \n# df_wit_2 = pd.DataFrame({'img_path': img_path, 'class': caption})    ","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.274557Z","iopub.execute_input":"2022-10-04T02:32:46.279907Z","iopub.status.idle":"2022-10-04T02:32:46.285682Z","shell.execute_reply.started":"2022-10-04T02:32:46.279848Z","shell.execute_reply":"2022-10-04T02:32:46.284068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# root_path = \"../input/ab-wit-ds-3\"\n\n# for root, dirnames, filenames in os.walk(root_path):\n#     dir_names = dirnames\n#     break\n    \n# img_path = []\n# caption = []\n# for dir in dir_names:\n#     for f_name in glob(os.path.join(root_path, dir, '00000', '*.json')):\n#         df_json = pd.read_json(f_name, typ='series') #os.path.join(root_path, dir, '00000', f_name)\n#         if df_json['status'] == \"success\":\n#             img_path.append(f_name.strip().split(\".\")[2] + '.jpg')\n#             caption.append(df_json['caption'])\n        \n# df_wit_3 = pd.DataFrame({'img_path': img_path, 'class': caption})    ","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.288589Z","iopub.execute_input":"2022-10-04T02:32:46.28972Z","iopub.status.idle":"2022-10-04T02:32:46.300391Z","shell.execute_reply.started":"2022-10-04T02:32:46.289508Z","shell.execute_reply":"2022-10-04T02:32:46.299394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# root_path = \"../input/ab-wit-ds-4\"\n\n# for root, dirnames, filenames in os.walk(root_path):\n#     dir_names = dirnames\n#     break\n    \n# img_path = []\n# caption = []\n# for dir in dir_names:\n#     for f_name in glob(os.path.join(root_path, dir, '00000', '*.json')):\n#         df_json = pd.read_json(f_name, typ='series') #os.path.join(root_path, dir, '00000', f_name)\n#         if df_json['status'] == \"success\":\n#             img_path.append(f_name.strip().split(\".\")[2] + '.jpg')\n#             caption.append(df_json['caption'])\n        \n# df_wit_4 = pd.DataFrame({'img_path': img_path, 'class': caption})    ","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.301441Z","iopub.execute_input":"2022-10-04T02:32:46.30175Z","iopub.status.idle":"2022-10-04T02:32:46.315242Z","shell.execute_reply.started":"2022-10-04T02:32:46.301724Z","shell.execute_reply":"2022-10-04T02:32:46.314317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# root_path = \"../input/ab-wit-ds-5\"\n\n# for root, dirnames, filenames in os.walk(root_path):\n#     dir_names = dirnames\n#     break\n    \n# img_path = []\n# caption = []\n# for dir in dir_names:\n#     for f_name in glob(os.path.join(root_path, dir, '00000', '*.json')):\n#         df_json = pd.read_json(f_name, typ='series') #os.path.join(root_path, dir, '00000', f_name)\n#         if df_json['status'] == \"success\":\n#             img_path.append(f_name.strip().split(\".\")[2] + '.jpg')\n#             caption.append(df_json['caption'])\n        \n# df_wit_5 = pd.DataFrame({'img_path': img_path, 'class': caption})    ","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.316585Z","iopub.execute_input":"2022-10-04T02:32:46.316987Z","iopub.status.idle":"2022-10-04T02:32:46.326365Z","shell.execute_reply.started":"2022-10-04T02:32:46.316961Z","shell.execute_reply":"2022-10-04T02:32:46.325401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_wit = pd.concat([\n#     df_wit_1,\n#     df_wit_2,\n#     df_wit_3,\n#     df_wit_4,\n#     df_wit_5\n# ], axis=0)\n\n# df_wit.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.327623Z","iopub.execute_input":"2022-10-04T02:32:46.328282Z","iopub.status.idle":"2022-10-04T02:32:46.338177Z","shell.execute_reply.started":"2022-10-04T02:32:46.328248Z","shell.execute_reply":"2022-10-04T02:32:46.337305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wit = df_wit_1.copy()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.339702Z","iopub.execute_input":"2022-10-04T02:32:46.340069Z","iopub.status.idle":"2022-10-04T02:32:46.349966Z","shell.execute_reply.started":"2022-10-04T02:32:46.340035Z","shell.execute_reply":"2022-10-04T02:32:46.34875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wit['class'] = df_wit['class'].str.replace(\"List of \", \"\")\ndf_wit.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.351747Z","iopub.execute_input":"2022-10-04T02:32:46.352067Z","iopub.status.idle":"2022-10-04T02:32:46.374679Z","shell.execute_reply.started":"2022-10-04T02:32:46.352041Z","shell.execute_reply":"2022-10-04T02:32:46.373253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wit['class'].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.376237Z","iopub.execute_input":"2022-10-04T02:32:46.37664Z","iopub.status.idle":"2022-10-04T02:32:46.387023Z","shell.execute_reply.started":"2022-10-04T02:32:46.376607Z","shell.execute_reply":"2022-10-04T02:32:46.385646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wit['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.38868Z","iopub.execute_input":"2022-10-04T02:32:46.389492Z","iopub.status.idle":"2022-10-04T02:32:46.400595Z","shell.execute_reply.started":"2022-10-04T02:32:46.389466Z","shell.execute_reply":"2022-10-04T02:32:46.399553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counts = df_wit['class'].value_counts()\n\ndf_wit = df_wit[~df_wit['class'].isin(counts[counts < 2].index)]\ndf_wit.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.402337Z","iopub.execute_input":"2022-10-04T02:32:46.402682Z","iopub.status.idle":"2022-10-04T02:32:46.41848Z","shell.execute_reply.started":"2022-10-04T02:32:46.40264Z","shell.execute_reply":"2022-10-04T02:32:46.417468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wit['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.420057Z","iopub.execute_input":"2022-10-04T02:32:46.420391Z","iopub.status.idle":"2022-10-04T02:32:46.430218Z","shell.execute_reply.started":"2022-10-04T02:32:46.420358Z","shell.execute_reply":"2022-10-04T02:32:46.429176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wit['class'].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.431642Z","iopub.execute_input":"2022-10-04T02:32:46.432454Z","iopub.status.idle":"2022-10-04T02:32:46.440164Z","shell.execute_reply.started":"2022-10-04T02:32:46.43242Z","shell.execute_reply":"2022-10-04T02:32:46.439309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wit['img_path'] = df_wit['img_path'].str.replace(\"/input\", \"/kaggle/input\")\ndf_wit.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.441517Z","iopub.execute_input":"2022-10-04T02:32:46.442282Z","iopub.status.idle":"2022-10-04T02:32:46.462695Z","shell.execute_reply.started":"2022-10-04T02:32:46.442247Z","shell.execute_reply":"2022-10-04T02:32:46.4619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# exclude = []\n# for idx, row in df_wit.iterrows():\n#     try:\n#         Image.open(row[\"img_path\"])\n#     except:\n#         exclude.append(row[\"img_path\"])","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.464813Z","iopub.execute_input":"2022-10-04T02:32:46.465415Z","iopub.status.idle":"2022-10-04T02:32:46.46928Z","shell.execute_reply.started":"2022-10-04T02:32:46.465382Z","shell.execute_reply":"2022-10-04T02:32:46.468193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# exclude","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.470729Z","iopub.execute_input":"2022-10-04T02:32:46.471095Z","iopub.status.idle":"2022-10-04T02:32:46.479655Z","shell.execute_reply.started":"2022-10-04T02:32:46.471063Z","shell.execute_reply":"2022-10-04T02:32:46.478725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# imagenet mini","metadata":{}},{"cell_type":"code","source":"# import glob","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.481044Z","iopub.execute_input":"2022-10-04T02:32:46.481391Z","iopub.status.idle":"2022-10-04T02:32:46.490652Z","shell.execute_reply.started":"2022-10-04T02:32:46.481357Z","shell.execute_reply":"2022-10-04T02:32:46.48968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # n_imagenetmini = 5000\n# INPUT_DIR = '/kaggle/input'\n# imagenet_cls_map = {'n02119789': 'kit fox, Vulpes macrotis', 'n02100735': 'English setter', 'n02096294': 'Australian terrier', 'n02066245': 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus', 'n02509815': 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens', 'n02124075': 'Egyptian cat', 'n02417914': 'ibex, Capra ibex', 'n02123394': 'Persian cat', 'n02125311': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 'n02423022': 'gazelle', 'n02346627': 'porcupine, hedgehog', 'n02077923': 'sea lion', 'n02447366': 'badger', 'n02109047': 'Great Dane', 'n02092002': 'Scottish deerhound, deerhound', 'n02071294': 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca', 'n02442845': 'mink', 'n02504458': 'African elephant, Loxodonta africana', 'n02114712': 'red wolf, maned wolf, Canis rufus, Canis niger', 'n02128925': 'jaguar, panther, Panthera onca, Felis onca', 'n02117135': 'hyena, hyaena', 'n02493509': 'titi, titi monkey', 'n02457408': 'three-toed sloth, ai, Bradypus tridactylus', 'n02389026': 'sorrel', 'n02443484': 'black-footed ferret, ferret, Mustela nigripes', 'n02110341': 'dalmatian, coach dog, carriage dog', 'n02093256': 'Staffordshire bullterrier, Staffordshire bull terrier', 'n02106382': 'Bouvier des Flandres, Bouviers des Flandres', 'n02441942': 'weasel', 'n02113712': 'miniature poodle', 'n02415577': 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis', 'n02356798': 'fox squirrel, eastern fox squirrel, Sciurus niger', 'n02488702': 'colobus, colobus monkey', 'n02123159': 'tiger cat', 'n02422699': 'impala, Aepyceros melampus', 'n02114855': 'coyote, prairie wolf, brush wolf, Canis latrans', 'n02094433': 'Yorkshire terrier', 'n02111277': 'Newfoundland, Newfoundland dog', 'n02119022': 'red fox, Vulpes vulpes', 'n02422106': 'hartebeest', 'n02120505': 'grey fox, gray fox, Urocyon cinereoargenteus', 'n02086079': 'Pekinese, Pekingese, Peke', 'n02484975': 'guenon, guenon monkey', 'n02137549': 'mongoose', 'n02500267': 'indri, indris, Indri indri, Indri brevicaudatus', 'n02129604': 'tiger, Panthera tigris', 'n02396427': 'wild boar, boar, Sus scrofa', 'n02391049': 'zebra', 'n02412080': 'ram, tup', 'n02480495': 'orangutan, orang, orangutang, Pongo pygmaeus', 'n02110806': 'basenji', 'n02128385': 'leopard, Panthera pardus', 'n02100583': 'vizsla, Hungarian pointer', 'n02494079': 'squirrel monkey, Saimiri sciureus', 'n02123597': 'Siamese cat, Siamese', 'n02481823': 'chimpanzee, chimp, Pan troglodytes', 'n02105505': 'komondor', 'n02489166': 'proboscis monkey, Nasalis larvatus', 'n02364673': 'guinea pig, Cavia cobaya', 'n02114548': 'white wolf, Arctic wolf, Canis lupus tundrarum', 'n02134084': 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus', 'n02480855': 'gorilla, Gorilla gorilla', 'n02403003': 'ox', 'n02108551': 'Tibetan mastiff', 'n02493793': 'spider monkey, Ateles geoffroyi', 'n02107142': 'Doberman, Doberman pinscher', 'n02397096': 'warthog', 'n02437312': 'Arabian camel, dromedary, Camelus dromedarius', 'n02483708': 'siamang, Hylobates syndactylus, Symphalangus syndactylus', 'n02099601': 'golden retriever', 'n02106166': 'Border collie', 'n02326432': 'hare', 'n02108089': 'boxer', 'n02486261': 'patas, hussar monkey, Erythrocebus patas', 'n02486410': 'baboon', 'n02487347': 'macaque', 'n02492035': 'capuchin, ringtail, Cebus capucinus', 'n02099267': 'flat-coated retriever', 'n02395406': 'hog, pig, grunter, squealer, Sus scrofa', 'n02109961': 'Eskimo dog, husky', 'n02101388': 'Brittany spaniel', 'n03187595': 'dial telephone, dial phone', 'n03733281': 'maze, labyrinth', 'n02101006': 'Gordon setter', 'n02115641': 'dingo, warrigal, warragal, Canis dingo', 'n02342885': 'hamster', 'n02120079': 'Arctic fox, white fox, Alopex lagopus', 'n02408429': 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis', 'n02133161': 'American black bear, black bear, Ursus americanus, Euarctos americanus', 'n02328150': 'Angora, Angora rabbit', 'n02410509': 'bison', 'n02492660': 'howler monkey, howler', 'n02398521': 'hippopotamus, hippo, river horse, Hippopotamus amphibius', 'n02510455': 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca', 'n02123045': 'tabby, tabby cat', 'n02490219': 'marmoset', 'n02109525': 'Saint Bernard, St Bernard', 'n02454379': 'armadillo', 'n02090379': 'redbone', 'n02443114': 'polecat, fitch, foulmart, foumart, Mustela putorius', 'n02361337': 'marmot', 'n02483362': 'gibbon, Hylobates lar', 'n02437616': 'llama', 'n02325366': 'wood rabbit, cottontail, cottontail rabbit', 'n02129165': 'lion, king of beasts, Panthera leo', 'n02100877': 'Irish setter, red setter', 'n02074367': 'dugong, Dugong dugon', 'n02504013': 'Indian elephant, Elephas maximus', 'n02363005': 'beaver', 'n02497673': 'Madagascar cat, ring-tailed lemur, Lemur catta', 'n02087394': 'Rhodesian ridgeback', 'n02127052': 'lynx, catamount', 'n02116738': 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus', 'n02488291': 'langur', 'n02114367': 'timber wolf, grey wolf, gray wolf, Canis lupus', 'n02130308': 'cheetah, chetah, Acinonyx jubatus', 'n02134418': 'sloth bear, Melursus ursinus, Ursus ursinus', 'n02106662': 'German shepherd, German shepherd dog, German police dog, alsatian', 'n02444819': 'otter', 'n01882714': 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus', 'n01871265': 'tusker', 'n01872401': 'echidna, spiny anteater, anteater', 'n01877812': 'wallaby, brush kangaroo', 'n01873310': 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus', 'n01883070': 'wombat', 'n04086273': 'revolver, six-gun, six-shooter', 'n04507155': 'umbrella', 'n04147183': 'schooner', 'n04254680': 'soccer ball', 'n02672831': 'accordion, piano accordion, squeeze box', 'n02219486': 'ant, emmet, pismire', 'n02317335': 'starfish, sea star', 'n01968897': 'chambered nautilus, pearly nautilus, nautilus', 'n03452741': 'grand piano, grand', 'n03642806': 'laptop, laptop computer', 'n07745940': 'strawberry', 'n02690373': 'airliner', 'n04552348': 'warplane, military plane', 'n02692877': 'airship, dirigible', 'n02782093': 'balloon', 'n04266014': 'space shuttle', 'n03344393': 'fireboat', 'n03447447': 'gondola', 'n04273569': 'speedboat', 'n03662601': 'lifeboat', 'n02951358': 'canoe', 'n04612504': 'yawl', 'n02981792': 'catamaran', 'n04483307': 'trimaran', 'n03095699': 'container ship, containership, container vessel', 'n03673027': 'liner, ocean liner', 'n03947888': 'pirate, pirate ship', 'n02687172': 'aircraft carrier, carrier, flattop, attack aircraft carrier', 'n04347754': 'submarine, pigboat, sub, U-boat', 'n04606251': 'wreck', 'n03478589': 'half track', 'n04389033': 'tank, army tank, armored combat vehicle, armoured combat vehicle', 'n03773504': 'missile', 'n02860847': 'bobsled, bobsleigh, bob', 'n03218198': 'dogsled, dog sled, dog sleigh', 'n02835271': 'bicycle-built-for-two, tandem bicycle, tandem', 'n03792782': 'mountain bike, all-terrain bike, off-roader', 'n03393912': 'freight car', 'n03895866': 'passenger car, coach, carriage', 'n02797295': 'barrow, garden cart, lawn cart, wheelbarrow', 'n04204347': 'shopping cart', 'n03791053': 'motor scooter, scooter', 'n03384352': 'forklift', 'n03272562': 'electric locomotive', 'n04310018': 'steam locomotive', 'n02704792': 'amphibian, amphibious vehicle', 'n02701002': 'ambulance', 'n02814533': 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon', 'n02930766': 'cab, hack, taxi, taxicab', 'n03100240': 'convertible', 'n03594945': 'jeep, landrover', 'n03670208': 'limousine, limo', 'n03770679': 'minivan', 'n03777568': 'Model T', 'n04037443': 'racer, race car, racing car', 'n04285008': 'sports car, sport car', 'n03444034': 'go-kart', 'n03445924': 'golfcart, golf cart', 'n03785016': 'moped', 'n04252225': 'snowplow, snowplough', 'n03345487': 'fire engine, fire truck', 'n03417042': 'garbage truck, dustcart', 'n03930630': 'pickup, pickup truck', 'n04461696': 'tow truck, tow car, wrecker', 'n04467665': 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi', 'n03796401': 'moving van', 'n03977966': 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria', 'n04065272': 'recreational vehicle, RV, R.V.', 'n04335435': 'streetcar, tram, tramcar, trolley, trolley car', 'n04252077': 'snowmobile', 'n04465501': 'tractor', 'n03776460': 'mobile home, manufactured home', 'n04482393': 'tricycle, trike, velocipede', 'n04509417': 'unicycle, monocycle', 'n03538406': 'horse cart, horse-cart', 'n03788365': 'mosquito net', 'n03868242': 'oxcart', 'n02804414': 'bassinet', 'n03125729': 'cradle', 'n03131574': 'crib, cot', 'n03388549': 'four-poster', 'n02870880': 'bookcase', 'n03018349': 'china cabinet, china closet', 'n03742115': 'medicine chest, medicine cabinet', 'n03016953': 'chiffonier, commode', 'n04380533': 'table lamp', 'n03337140': 'file, file cabinet, filing cabinet', 'n03902125': 'pay-phone, pay-station', 'n03891251': 'park bench', 'n02791124': 'barber chair', 'n04429376': 'throne', 'n03376595': 'folding chair', 'n04099969': 'rocking chair, rocker', 'n04344873': 'studio couch, day bed', 'n04447861': 'toilet seat', 'n03179701': 'desk', 'n03982430': 'pool table, billiard table, snooker table', 'n03201208': 'dining table, board', 'n03290653': 'entertainment center', 'n04550184': 'wardrobe, closet, press', 'n07742313': 'Granny Smith', 'n07747607': 'orange', 'n07749582': 'lemon', 'n07753113': 'fig', 'n07753275': 'pineapple, ananas', 'n07753592': 'banana', 'n07754684': 'jackfruit, jak, jack', 'n07760859': 'custard apple', 'n07768694': 'pomegranate', 'n12267677': 'acorn', 'n12620546': 'hip, rose hip, rosehip', 'n13133613': 'ear, spike, capitulum', 'n11879895': 'rapeseed', 'n12144580': 'corn', 'n12768682': 'buckeye, horse chestnut, conker', 'n03854065': 'organ, pipe organ', 'n04515003': 'upright, upright piano', 'n03017168': 'chime, bell, gong', 'n03249569': 'drum, membranophone, tympan', 'n03447721': 'gong, tam-tam', 'n03720891': 'maraca', 'n03721384': 'marimba, xylophone', 'n04311174': 'steel drum', 'n02787622': 'banjo', 'n02992211': 'cello, violoncello', 'n03637318': 'lampshade, lamp shade', 'n03495258': 'harp', 'n02676566': 'acoustic guitar', 'n03272010': 'electric guitar', 'n03110669': 'cornet, horn, trumpet, trump', 'n03394916': 'French horn, horn', 'n04487394': 'trombone', 'n03494278': 'harmonica, mouth organ, harp, mouth harp', 'n03840681': 'ocarina, sweet potato', 'n03884397': 'panpipe, pandean pipe, syrinx', 'n02804610': 'bassoon', 'n04141076': 'sax, saxophone', 'n03372029': 'flute, transverse flute', 'n11939491': 'daisy', 'n12057211': \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\", 'n09246464': 'cliff, drop, drop-off', 'n09468604': 'valley, vale', 'n09193705': 'alp', 'n09472597': 'volcano', 'n09399592': 'promontory, headland, head, foreland', 'n09421951': 'sandbar, sand bar', 'n09256479': 'coral reef', 'n09332890': 'lakeside, lakeshore', 'n09428293': 'seashore, coast, seacoast, sea-coast', 'n09288635': 'geyser', 'n03498962': 'hatchet', 'n03041632': 'cleaver, meat cleaver, chopper', 'n03658185': 'letter opener, paper knife, paperknife', 'n03954731': \"plane, carpenter's plane, woodworking plane\", 'n03995372': 'power drill', 'n03649909': 'lawn mower, mower', 'n03481172': 'hammer', 'n03109150': 'corkscrew, bottle screw', 'n02951585': 'can opener, tin opener', 'n03970156': \"plunger, plumber's helper\", 'n04154565': 'screwdriver', 'n04208210': 'shovel', 'n03967562': 'plow, plough', 'n03000684': 'chain saw, chainsaw', 'n01514668': 'cock', 'n01514859': 'hen', 'n01518878': 'ostrich, Struthio camelus', 'n01530575': 'brambling, Fringilla montifringilla', 'n01531178': 'goldfinch, Carduelis carduelis', 'n01532829': 'house finch, linnet, Carpodacus mexicanus', 'n01534433': 'junco, snowbird', 'n01537544': 'indigo bunting, indigo finch, indigo bird, Passerina cyanea', 'n01558993': 'robin, American robin, Turdus migratorius', 'n01560419': 'bulbul', 'n01580077': 'jay', 'n01582220': 'magpie', 'n01592084': 'chickadee', 'n01601694': 'water ouzel, dipper', 'n01608432': 'kite', 'n01614925': 'bald eagle, American eagle, Haliaeetus leucocephalus', 'n01616318': 'vulture', 'n01622779': 'great grey owl, great gray owl, Strix nebulosa', 'n01795545': 'black grouse', 'n01796340': 'ptarmigan', 'n01797886': 'ruffed grouse, partridge, Bonasa umbellus', 'n01798484': 'prairie chicken, prairie grouse, prairie fowl', 'n01806143': 'peacock', 'n01806567': 'quail', 'n01807496': 'partridge', 'n01817953': 'African grey, African gray, Psittacus erithacus', 'n01818515': 'macaw', 'n01819313': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita', 'n01820546': 'lorikeet', 'n01824575': 'coucal', 'n01828970': 'bee eater', 'n01829413': 'hornbill', 'n01833805': 'hummingbird', 'n01843065': 'jacamar', 'n01843383': 'toucan', 'n01847000': 'drake', 'n01855032': 'red-breasted merganser, Mergus serrator', 'n01855672': 'goose', 'n01860187': 'black swan, Cygnus atratus', 'n02002556': 'white stork, Ciconia ciconia', 'n02002724': 'black stork, Ciconia nigra', 'n02006656': 'spoonbill', 'n02007558': 'flamingo', 'n02009912': 'American egret, great white heron, Egretta albus', 'n02009229': 'little blue heron, Egretta caerulea', 'n02011460': 'bittern', 'n02012849': 'crane', 'n02013706': 'limpkin, Aramus pictus', 'n02018207': 'American coot, marsh hen, mud hen, water hen, Fulica americana', 'n02018795': 'bustard', 'n02025239': 'ruddy turnstone, Arenaria interpres', 'n02027492': 'red-backed sandpiper, dunlin, Erolia alpina', 'n02028035': 'redshank, Tringa totanus', 'n02033041': 'dowitcher', 'n02037110': 'oystercatcher, oyster catcher', 'n02017213': 'European gallinule, Porphyrio porphyrio', 'n02051845': 'pelican', 'n02056570': 'king penguin, Aptenodytes patagonica', 'n02058221': 'albatross, mollymawk', 'n01484850': 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias', 'n01491361': 'tiger shark, Galeocerdo cuvieri', 'n01494475': 'hammerhead, hammerhead shark', 'n01496331': 'electric ray, crampfish, numbfish, torpedo', 'n01498041': 'stingray', 'n02514041': 'barracouta, snoek', 'n02536864': 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch', 'n01440764': 'tench, Tinca tinca', 'n01443537': 'goldfish, Carassius auratus', 'n02526121': 'eel', 'n02606052': 'rock beauty, Holocanthus tricolor', 'n02607072': 'anemone fish', 'n02643566': 'lionfish', 'n02655020': 'puffer, pufferfish, blowfish, globefish', 'n02640242': 'sturgeon', 'n02641379': 'gar, garfish, garpike, billfish, Lepisosteus osseus', 'n01664065': 'loggerhead, loggerhead turtle, Caretta caretta', 'n01667114': 'mud turtle', 'n01667778': 'terrapin', 'n01669191': 'box turtle, box tortoise', 'n01675722': 'banded gecko', 'n01677366': 'common iguana, iguana, Iguana iguana', 'n01682714': 'American chameleon, anole, Anolis carolinensis', 'n01685808': 'whiptail, whiptail lizard', 'n01687978': 'agama', 'n01688243': 'frilled lizard, Chlamydosaurus kingi', 'n01689811': 'alligator lizard', 'n01692333': 'Gila monster, Heloderma suspectum', 'n01693334': 'green lizard, Lacerta viridis', 'n01694178': 'African chameleon, Chamaeleo chamaeleon', 'n01695060': 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis', 'n01704323': 'triceratops', 'n01697457': 'African crocodile, Nile crocodile, Crocodylus niloticus', 'n01698640': 'American alligator, Alligator mississipiensis', 'n01728572': 'thunder snake, worm snake, Carphophis amoenus', 'n01728920': 'ringneck snake, ring-necked snake, ring snake', 'n01729322': 'hognose snake, puff adder, sand viper', 'n01729977': 'green snake, grass snake', 'n01734418': 'king snake, kingsnake', 'n01735189': 'garter snake, grass snake', 'n01737021': 'water snake', 'n01739381': 'vine snake', 'n01740131': 'night snake, Hypsiglena torquata', 'n01742172': 'boa constrictor, Constrictor constrictor', 'n01744401': 'rock python, rock snake, Python sebae', 'n01748264': 'Indian cobra, Naja naja', 'n01749939': 'green mamba', 'n01751748': 'sea snake', 'n01753488': 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus', 'n04326547': 'stone wall', 'n01756291': 'sidewinder, horned rattlesnake, Crotalus cerastes', 'n01629819': 'European fire salamander, Salamandra salamandra', 'n01630670': 'common newt, Triturus vulgaris', 'n01631663': 'eft', 'n01632458': 'spotted salamander, Ambystoma maculatum', 'n01632777': 'axolotl, mud puppy, Ambystoma mexicanum', 'n01641577': 'bullfrog, Rana catesbeiana', 'n01644373': 'tree frog, tree-frog', 'n01644900': 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui', 'n04579432': 'whistle', 'n04592741': 'wing', 'n03876231': 'paintbrush', 'n03868863': 'oxygen mask', 'n04251144': 'snorkel', 'n03691459': 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system', 'n03759954': 'microphone, mike', 'n04152593': 'screen, CRT screen', 'n03793489': 'mouse, computer mouse', 'n03271574': 'electric fan, blower', 'n03843555': 'oil filter', 'n04332243': 'strainer', 'n04265275': 'space heater', 'n04330267': 'stove', 'n03467068': 'guillotine', 'n02794156': 'barometer', 'n04118776': 'rule, ruler', 'n03841143': 'odometer, hodometer, mileometer, milometer', 'n04141975': 'scale, weighing machine', 'n02708093': 'analog clock', 'n03196217': 'digital clock', 'n04548280': 'wall clock', 'n03544143': 'hourglass', 'n04355338': 'sundial', 'n03891332': 'parking meter', 'n04328186': 'stopwatch, stop watch', 'n03197337': 'digital watch', 'n04317175': 'stethoscope', 'n04376876': 'syringe', 'n03706229': 'magnetic compass', 'n02841315': 'binoculars, field glasses, opera glasses', 'n04009552': 'projector', 'n04356056': 'sunglasses, dark glasses, shades', 'n03692522': \"loupe, jeweler's loupe\", 'n04044716': 'radio telescope, radio reflector', 'n02879718': 'bow', 'n02950826': 'cannon', 'n02749479': 'assault rifle, assault gun', 'n04090263': 'rifle', 'n04008634': 'projectile, missile', 'n03085013': 'computer keyboard, keypad', 'n04505470': 'typewriter keyboard', 'n03126707': 'crane', 'n03666591': 'lighter, light, igniter, ignitor', 'n02666196': 'abacus', 'n02977058': 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM', 'n04238763': 'slide rule, slipstick', 'n03180011': 'desktop computer', 'n03485407': 'hand-held computer, hand-held microcomputer', 'n03832673': 'notebook, notebook computer', 'n03874599': 'padlock', 'n03496892': 'harvester, reaper', 'n04428191': 'thresher, thrasher, threshing machine', 'n04004767': 'printer', 'n04243546': 'slot, one-armed bandit', 'n04525305': 'vending machine', 'n04179913': 'sewing machine', 'n03602883': 'joystick', 'n04372370': 'switch, electric switch, electrical switch', 'n03532672': 'hook, claw', 'n02974003': 'car wheel', 'n03874293': 'paddlewheel, paddle wheel', 'n03944341': 'pinwheel', 'n03992509': \"potter's wheel\", 'n03425413': 'gas pump, gasoline pump, petrol pump, island dispenser', 'n02966193': 'carousel, carrousel, merry-go-round, roundabout, whirligig', 'n04371774': 'swing', 'n04067472': 'reel', 'n04040759': 'radiator', 'n04019541': 'puck, hockey puck', 'n03492542': 'hard disc, hard disk, fixed disk', 'n04355933': 'sunglass', 'n03929660': 'pick, plectrum, plectron', 'n02965783': 'car mirror', 'n04258138': 'solar dish, solar collector, solar furnace', 'n04074963': 'remote control, remote', 'n03208938': 'disk brake, disc brake', 'n02910353': 'buckle', 'n03476684': 'hair slide', 'n03627232': 'knot', 'n03075370': 'combination lock', 'n06359193': 'web site, website, internet site, site', 'n03804744': 'nail', 'n04127249': 'safety pin', 'n04153751': 'screw', 'n03803284': 'muzzle', 'n04162706': 'seat belt, seatbelt', 'n04228054': 'ski', 'n02948072': 'candle, taper, wax light', 'n03590841': \"jack-o'-lantern\", 'n04286575': 'spotlight, spot', 'n04456115': 'torch', 'n03814639': 'neck brace', 'n03933933': 'pier', 'n04485082': 'tripod', 'n03733131': 'maypole', 'n03483316': 'hand blower, blow dryer, blow drier, hair dryer, hair drier', 'n03794056': 'mousetrap', 'n04275548': \"spider web, spider's web\", 'n01768244': 'trilobite', 'n01770081': 'harvestman, daddy longlegs, Phalangium opilio', 'n01770393': 'scorpion', 'n01773157': 'black and gold garden spider, Argiope aurantia', 'n01773549': 'barn spider, Araneus cavaticus', 'n01773797': 'garden spider, Aranea diademata', 'n01774384': 'black widow, Latrodectus mactans', 'n01774750': 'tarantula', 'n01775062': 'wolf spider, hunting spider', 'n01776313': 'tick', 'n01784675': 'centipede', 'n01990800': 'isopod', 'n01978287': 'Dungeness crab, Cancer magister', 'n01978455': 'rock crab, Cancer irroratus', 'n01980166': 'fiddler crab', 'n01981276': 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica', 'n01983481': 'American lobster, Northern lobster, Maine lobster, Homarus americanus', 'n01984695': 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish', 'n01985128': 'crayfish, crawfish, crawdad, crawdaddy', 'n01986214': 'hermit crab', 'n02165105': 'tiger beetle', 'n02165456': 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle', 'n02167151': 'ground beetle, carabid beetle', 'n02168699': 'long-horned beetle, longicorn, longicorn beetle', 'n02169497': 'leaf beetle, chrysomelid', 'n02172182': 'dung beetle', 'n02174001': 'rhinoceros beetle', 'n02177972': 'weevil', 'n02190166': 'fly', 'n02206856': 'bee', 'n02226429': 'grasshopper, hopper', 'n02229544': 'cricket', 'n02231487': 'walking stick, walkingstick, stick insect', 'n02233338': 'cockroach, roach', 'n02236044': 'mantis, mantid', 'n02256656': 'cicada, cicala', 'n02259212': 'leafhopper', 'n02264363': 'lacewing, lacewing fly', 'n02268443': \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\", 'n02268853': 'damselfly', 'n02276258': 'admiral', 'n02277742': 'ringlet, ringlet butterfly', 'n02279972': 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus', 'n02280649': 'cabbage butterfly', 'n02281406': 'sulphur butterfly, sulfur butterfly', 'n02281787': 'lycaenid, lycaenid butterfly', 'n01910747': 'jellyfish', 'n01914609': 'sea anemone, anemone', 'n01917289': 'brain coral', 'n01924916': 'flatworm, platyhelminth', 'n01930112': 'nematode, nematode worm, roundworm', 'n01943899': 'conch', 'n01944390': 'snail', 'n01945685': 'slug', 'n01950731': 'sea slug, nudibranch', 'n01955084': 'chiton, coat-of-mail shell, sea cradle, polyplacophore', 'n02319095': 'sea urchin', 'n02321529': 'sea cucumber, holothurian', 'n03584829': 'iron, smoothing iron', 'n03297495': 'espresso maker', 'n03761084': 'microwave, microwave oven', 'n03259280': 'Dutch oven', 'n04111531': 'rotisserie', 'n04442312': 'toaster', 'n04542943': 'waffle iron', 'n04517823': 'vacuum, vacuum cleaner', 'n03207941': 'dishwasher, dish washer, dishwashing machine', 'n04070727': 'refrigerator, icebox', 'n04554684': 'washer, automatic washer, washing machine', 'n03133878': 'Crock Pot', 'n03400231': 'frying pan, frypan, skillet', 'n04596742': 'wok', 'n02939185': 'caldron, cauldron', 'n03063689': 'coffeepot', 'n04398044': 'teapot', 'n04270147': 'spatula', 'n02699494': 'altar', 'n04486054': 'triumphal arch', 'n03899768': 'patio, terrace', 'n04311004': 'steel arch bridge', 'n04366367': 'suspension bridge', 'n04532670': 'viaduct', 'n02793495': 'barn', 'n03457902': 'greenhouse, nursery, glasshouse', 'n03877845': 'palace', 'n03781244': 'monastery', 'n03661043': 'library', 'n02727426': 'apiary, bee house', 'n02859443': 'boathouse', 'n03028079': 'church, church building', 'n03788195': 'mosque', 'n04346328': 'stupa, tope', 'n03956157': 'planetarium', 'n04081281': 'restaurant, eating house, eating place, eatery', 'n03032252': 'cinema, movie theater, movie theatre, movie house, picture palace', 'n03529860': 'home theater, home theatre', 'n03697007': 'lumbermill, sawmill', 'n03065424': 'coil, spiral, volute, whorl, helix', 'n03837869': 'obelisk', 'n04458633': 'totem pole', 'n02980441': 'castle', 'n04005630': 'prison, prison house', 'n03461385': 'grocery store, grocery, food market, market', 'n02776631': 'bakery, bakeshop, bakehouse', 'n02791270': 'barbershop', 'n02871525': 'bookshop, bookstore, bookstall', 'n02927161': 'butcher shop, meat market', 'n03089624': 'confectionery, confectionary, candy store', 'n04200800': 'shoe shop, shoe-shop, shoe store', 'n04443257': 'tobacco shop, tobacconist shop, tobacconist', 'n04462240': 'toyshop', 'n03388043': 'fountain', 'n03042490': 'cliff dwelling', 'n04613696': 'yurt', 'n03216828': 'dock, dockage, docking facility', 'n02892201': 'brass, memorial tablet, plaque', 'n03743016': 'megalith, megalithic structure', 'n02788148': 'bannister, banister, balustrade, balusters, handrail', 'n02894605': 'breakwater, groin, groyne, mole, bulwark, seawall, jetty', 'n03160309': 'dam, dike, dyke', 'n03000134': 'chainlink fence', 'n03930313': 'picket fence, paling', 'n04604644': 'worm fence, snake fence, snake-rail fence, Virginia fence', 'n01755581': 'diamondback, diamondback rattlesnake, Crotalus adamanteus', 'n03459775': 'grille, radiator grille', 'n04239074': 'sliding door', 'n04501370': 'turnstile', 'n03792972': 'mountain tent', 'n04149813': 'scoreboard', 'n03530642': 'honeycomb', 'n03961711': 'plate rack', 'n03903868': 'pedestal, plinth, footstall', 'n02814860': 'beacon, lighthouse, beacon light, pharos', 'n01665541': 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea', 'n07711569': 'mashed potato', 'n07720875': 'bell pepper', 'n07714571': 'head cabbage', 'n07714990': 'broccoli', 'n07715103': 'cauliflower', 'n07716358': 'zucchini, courgette', 'n07716906': 'spaghetti squash', 'n07717410': 'acorn squash', 'n07717556': 'butternut squash', 'n07718472': 'cucumber, cuke', 'n07718747': 'artichoke, globe artichoke', 'n07730033': 'cardoon', 'n07734744': 'mushroom', 'n04209239': 'shower curtain', 'n03594734': 'jean, blue jean, denim', 'n02971356': 'carton', 'n03485794': 'handkerchief, hankie, hanky, hankey', 'n04133789': 'sandal', 'n02747177': 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin', 'n04125021': 'safe', 'n07579787': 'plate', 'n03814906': 'necklace', 'n03134739': 'croquet ball', 'n03404251': 'fur coat', 'n04423845': 'thimble', 'n03877472': \"pajama, pyjama, pj's, jammies\", 'n04120489': 'running shoe', 'n03838899': 'oboe, hautboy, hautbois', 'n03062245': 'cocktail shaker', 'n03014705': 'chest', 'n03717622': 'manhole cover', 'n03777754': 'modem', 'n04493381': 'tub, vat', 'n04476259': 'tray', 'n02777292': 'balance beam, beam', 'n07693725': 'bagel, beigel', 'n04536866': 'violin, fiddle', 'n03998194': 'prayer rug, prayer mat', 'n03617480': 'kimono', 'n07590611': 'hot pot, hotpot', 'n04579145': 'whiskey jug', 'n03623198': 'knee pad', 'n07248320': 'book jacket, dust cover, dust jacket, dust wrapper', 'n04277352': 'spindle', 'n04229816': 'ski mask', 'n02823428': 'beer bottle', 'n03127747': 'crash helmet', 'n02877765': 'bottlecap', 'n04435653': 'tile roof', 'n03724870': 'mask', 'n03710637': 'maillot', 'n03920288': 'Petri dish', 'n03379051': 'football helmet', 'n02807133': 'bathing cap, swimming cap', 'n04399382': 'teddy, teddy bear', 'n03527444': 'holster', 'n03983396': 'pop bottle, soda bottle', 'n03924679': 'photocopier', 'n04532106': 'vestment', 'n06785654': 'crossword puzzle, crossword', 'n03445777': 'golf ball', 'n07613480': 'trifle', 'n04350905': 'suit, suit of clothes', 'n04562935': 'water tower', 'n03325584': 'feather boa, boa', 'n03045698': 'cloak', 'n07892512': 'red wine', 'n03250847': 'drumstick', 'n04192698': 'shield, buckler', 'n03026506': 'Christmas stocking', 'n03534580': 'hoopskirt, crinoline', 'n07565083': 'menu', 'n04296562': 'stage', 'n02869837': 'bonnet, poke bonnet', 'n07871810': 'meat loaf, meatloaf', 'n02799071': 'baseball', 'n03314780': 'face powder', 'n04141327': 'scabbard', 'n04357314': 'sunscreen, sunblock, sun blocker', 'n02823750': 'beer glass', 'n13052670': 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa', 'n07583066': 'guacamole', 'n04599235': 'wool, woolen, woollen', 'n07802026': 'hay', 'n02883205': 'bow tie, bow-tie, bowtie', 'n03709823': 'mailbag, postbag', 'n04560804': 'water jug', 'n02909870': 'bucket, pail', 'n03207743': 'dishrag, dishcloth', 'n04263257': 'soup bowl', 'n07932039': 'eggnog', 'n03786901': 'mortar', 'n04479046': 'trench coat', 'n03873416': 'paddle, boat paddle', 'n02999410': 'chain', 'n04367480': 'swab, swob, mop', 'n03775546': 'mixing bowl', 'n07875152': 'potpie', 'n04591713': 'wine bottle', 'n04201297': 'shoji', 'n02916936': 'bulletproof vest', 'n03240683': 'drilling platform, offshore rig', 'n02840245': 'binder, ring-binder', 'n02963159': 'cardigan', 'n04370456': 'sweatshirt', 'n03991062': 'pot, flowerpot', 'n02843684': 'birdhouse', 'n03599486': 'jinrikisha, ricksha, rickshaw', 'n03482405': 'hamper', 'n03942813': 'ping-pong ball', 'n03908618': 'pencil box, pencil case', 'n07584110': 'consomme', 'n02730930': 'apron', 'n04023962': 'punching bag, punch bag, punching ball, punchball', 'n02769748': 'backpack, back pack, knapsack, packsack, rucksack, haversack', 'n10148035': 'groom, bridegroom', 'n02817516': 'bearskin, busby, shako', 'n03908714': 'pencil sharpener', 'n02906734': 'broom', 'n02667093': 'abaya', 'n03787032': 'mortarboard', 'n03980874': 'poncho', 'n03141823': 'crutch', 'n03976467': 'Polaroid camera, Polaroid Land camera', 'n04264628': 'space bar', 'n07930864': 'cup', 'n04039381': 'racket, racquet', 'n06874185': 'traffic light, traffic signal, stoplight', 'n04033901': 'quill, quill pen', 'n04041544': 'radio, wireless', 'n02128757': 'snow leopard, ounce, Panthera uncia', 'n07860988': 'dough', 'n03146219': 'cuirass', 'n03763968': 'military uniform', 'n03676483': 'lipstick, lip rouge', 'n04209133': 'shower cap', 'n03782006': 'monitor', 'n03857828': 'oscilloscope, scope, cathode-ray oscilloscope, CRO', 'n03775071': 'mitten', 'n02892767': 'brassiere, bra, bandeau', 'n07684084': 'French loaf', 'n04522168': 'vase', 'n03764736': 'milk can', 'n04118538': 'rugby ball', 'n03887697': 'paper towel', 'n13044778': 'earthstar', 'n03291819': 'envelope', 'n03770439': 'miniskirt, mini', 'n03124170': 'cowboy hat, ten-gallon hat', 'n04487081': 'trolleybus, trolley coach, trackless trolley', 'n03916031': 'perfume, essence', 'n02808440': 'bathtub, bathing tub, bath, tub', 'n07697537': 'hotdog, hot dog, red hot', 'n12985857': 'coral fungus', 'n02917067': 'bullet train, bullet', 'n03938244': 'pillow', 'n15075141': 'toilet tissue, toilet paper, bathroom tissue', 'n02978881': 'cassette', 'n02966687': \"carpenter's kit, tool kit\", 'n03633091': 'ladle', 'n13040303': 'stinkhorn, carrion fungus', 'n03690938': 'lotion', 'n03476991': 'hair spray', 'n02669723': \"academic gown, academic robe, judge's robe\", 'n03220513': 'dome', 'n03127925': 'crate', 'n04584207': 'wig', 'n07880968': 'burrito', 'n03937543': 'pill bottle', 'n03000247': 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour', 'n04418357': 'theater curtain, theatre curtain', 'n04590129': 'window shade', 'n02795169': 'barrel, cask', 'n04553703': 'washbasin, handbasin, washbowl, lavabo, wash-hand basin', 'n02783161': 'ballpoint, ballpoint pen, ballpen, Biro', 'n02802426': 'basketball', 'n02808304': 'bath towel', 'n03124043': 'cowboy boot', 'n03450230': 'gown', 'n04589890': 'window screen', 'n12998815': 'agaric', 'n02113799': 'standard poodle', 'n02992529': 'cellular telephone, cellular phone, cellphone, cell, mobile phone', 'n03825788': 'nipple', 'n02790996': 'barbell', 'n03710193': 'mailbox, letter box', 'n03630383': 'lab coat, laboratory coat', 'n03347037': 'fire screen, fireguard', 'n03769881': 'minibus', 'n03871628': 'packet', 'n02132136': 'brown bear, bruin, Ursus arctos', 'n03976657': 'pole', 'n03535780': 'horizontal bar, high bar', 'n04259630': 'sombrero', 'n03929855': 'pickelhaube', 'n04049303': 'rain barrel', 'n04548362': 'wallet, billfold, notecase, pocketbook', 'n02979186': 'cassette player', 'n06596364': 'comic book', 'n03935335': 'piggy bank, penny bank', 'n06794110': 'street sign', 'n02825657': 'bell cote, bell cot', 'n03388183': 'fountain pen', 'n04591157': 'Windsor tie', 'n04540053': 'volleyball', 'n03866082': 'overskirt', 'n04136333': 'sarong', 'n04026417': 'purse', 'n02865351': 'bolo tie, bolo, bola tie, bola', 'n02834397': 'bib', 'n03888257': 'parachute, chute', 'n04235860': 'sleeping bag', 'n04404412': 'television, television system', 'n04371430': 'swimming trunks, bathing trunks', 'n03733805': 'measuring cup', 'n07920052': 'espresso', 'n07873807': 'pizza, pizza pie', 'n02895154': 'breastplate, aegis, egis', 'n04204238': 'shopping basket', 'n04597913': 'wooden spoon', 'n04131690': 'saltshaker, salt shaker', 'n07836838': 'chocolate sauce, chocolate syrup', 'n09835506': 'ballplayer, baseball player', 'n03443371': 'goblet', 'n13037406': 'gyromitra', 'n04336792': 'stretcher', 'n04557648': 'water bottle', 'n02445715': 'skunk, polecat, wood pussy', 'n04254120': 'soap dispenser', 'n03595614': 'jersey, T-shirt, tee shirt', 'n04146614': 'school bus', 'n03598930': 'jigsaw puzzle', 'n03958227': 'plastic bag', 'n04069434': 'reflex camera', 'n03188531': 'diaper, nappy, napkin', 'n02786058': 'Band Aid', 'n07615774': 'ice lolly, lolly, lollipop, popsicle', 'n04525038': 'velvet', 'n04409515': 'tennis ball', 'n03424325': 'gasmask, respirator, gas helmet', 'n03223299': 'doormat, welcome mat', 'n03680355': 'Loafer', 'n07614500': 'ice cream, icecream', 'n07695742': 'pretzel', 'n04033995': 'quilt, comforter, comfort, puff', 'n03710721': 'maillot, tank suit', 'n04392985': 'tape player', 'n03047690': 'clog, geta, patten, sabot', 'n03584254': 'iPod', 'n13054560': 'bolete', 'n02138441': 'meerkat, mierkat', 'n10565667': 'scuba diver', 'n03950228': 'pitcher, ewer', 'n03729826': 'matchstick', 'n02837789': 'bikini, two-piece', 'n04254777': 'sock', 'n02988304': 'CD player', 'n03657121': 'lens cap, lens cover', 'n04417672': 'thatch, thatched roof', 'n04523525': 'vault', 'n02815834': 'beaker', 'n09229709': 'bubble', 'n07697313': 'cheeseburger', 'n03888605': 'parallel bars, bars', 'n03355925': 'flagpole, flagstaff', 'n03063599': 'coffee mug', 'n04116512': 'rubber eraser, rubber, pencil eraser', 'n04325704': 'stole', 'n07831146': 'carbonara', 'n03255030': 'dumbbell', 'n02110185': 'Siberian husky', 'n02102040': 'English springer, English springer spaniel', 'n02110063': 'malamute, malemute, Alaskan malamute', 'n02089867': 'Walker hound, Walker foxhound', 'n02102177': 'Welsh springer spaniel', 'n02091134': 'whippet', 'n02092339': 'Weimaraner', 'n02098105': 'soft-coated wheaten terrier', 'n02096437': 'Dandie Dinmont, Dandie Dinmont terrier', 'n02105641': 'Old English sheepdog, bobtail', 'n02091635': 'otterhound, otter hound', 'n02088466': 'bloodhound, sleuthhound', 'n02096051': 'Airedale, Airedale terrier', 'n02097130': 'giant schnauzer', 'n02089078': 'black-and-tan coonhound', 'n02086910': 'papillon', 'n02113978': 'Mexican hairless', 'n02113186': 'Cardigan, Cardigan Welsh corgi', 'n02105162': 'malinois', 'n02098413': 'Lhasa, Lhasa apso', 'n02091467': 'Norwegian elkhound, elkhound', 'n02106550': 'Rottweiler', 'n02091831': 'Saluki, gazelle hound', 'n02104365': 'schipperke', 'n02112706': 'Brabancon griffon', 'n02098286': 'West Highland white terrier', 'n02095889': 'Sealyham terrier, Sealyham', 'n02090721': 'Irish wolfhound', 'n02108000': 'EntleBucher', 'n02108915': 'French bulldog', 'n02107683': 'Bernese mountain dog', 'n02085936': 'Maltese dog, Maltese terrier, Maltese', 'n02094114': 'Norfolk terrier', 'n02087046': 'toy terrier', 'n02096177': 'cairn, cairn terrier', 'n02105056': 'groenendael', 'n02101556': 'clumber, clumber spaniel', 'n02088094': 'Afghan hound, Afghan', 'n02085782': 'Japanese spaniel', 'n02090622': 'borzoi, Russian wolfhound', 'n02113624': 'toy poodle', 'n02093859': 'Kerry blue terrier', 'n02097298': 'Scotch terrier, Scottish terrier, Scottie', 'n02096585': 'Boston bull, Boston terrier', 'n02107574': 'Greater Swiss Mountain dog', 'n02107908': 'Appenzeller', 'n02086240': 'Shih-Tzu', 'n02102973': 'Irish water spaniel', 'n02112018': 'Pomeranian', 'n02093647': 'Bedlington terrier', 'n02097047': 'miniature schnauzer', 'n02106030': 'collie', 'n02093991': 'Irish terrier', 'n02110627': 'affenpinscher, monkey pinscher, monkey dog', 'n02097658': 'silky terrier, Sydney silky', 'n02088364': 'beagle', 'n02111129': 'Leonberg', 'n02100236': 'German short-haired pointer', 'n02115913': 'dhole, Cuon alpinus', 'n02099849': 'Chesapeake Bay retriever', 'n02108422': 'bull mastiff', 'n02104029': 'kuvasz', 'n02110958': 'pug, pug-dog', 'n02099429': 'curly-coated retriever', 'n02094258': 'Norwich terrier', 'n02112350': 'keeshond', 'n02095570': 'Lakeland terrier', 'n02097209': 'standard schnauzer', 'n02097474': 'Tibetan terrier, chrysanthemum dog', 'n02095314': 'wire-haired fox terrier', 'n02088238': 'basset, basset hound', 'n02112137': 'chow, chow chow', 'n02093428': 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier', 'n02105855': 'Shetland sheepdog, Shetland sheep dog, Shetland', 'n02111500': 'Great Pyrenees', 'n02085620': 'Chihuahua', 'n02099712': 'Labrador retriever', 'n02111889': 'Samoyed, Samoyede', 'n02088632': 'bluetick', 'n02105412': 'kelpie', 'n02107312': 'miniature pinscher', 'n02091032': 'Italian greyhound', 'n02102318': 'cocker spaniel, English cocker spaniel, cocker', 'n02102480': 'Sussex spaniel', 'n02113023': 'Pembroke, Pembroke Welsh corgi', 'n02086646': 'Blenheim spaniel', 'n02091244': 'Ibizan hound, Ibizan Podenco', 'n02089973': 'English foxhound', 'n02105251': 'briard', 'n02093754': 'Border terrier'}\n# tmp_df = pd.DataFrame({\"img_path\":glob.glob(os.path.join(INPUT_DIR, \"imagenetmini-1000\", \"imagenet-mini\", \"train\", \"**\", \"*.JPEG\"))})\n# tmp_df[\"cls\"] = tmp_df[\"img_path\"].apply(lambda x: imagenet_cls_map[x.rsplit(\"/\", 2)[-2]])\n# tmp_df[\"cls_cnt\"] = tmp_df.groupby(\"cls\")[\"cls\"].transform(\"count\")\n# tmp_df = tmp_df.sort_values(by=\"cls_cnt\", ascending=False)\n# # tmp_df = tmp_df.head(n_imagenetmini).sample(n_imagenetmini).reset_index(drop=True)\n# tmp_df = tmp_df[[\"img_path\", \"cls\"]]\n# tmp_df[\"supercls\"] = \"other\"\n# tmp_df[\"dataset\"] = \"imagenetmini-1000\"\n# # tmp_df[\"val_only\"] = False","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.492599Z","iopub.execute_input":"2022-10-04T02:32:46.493262Z","iopub.status.idle":"2022-10-04T02:32:46.517848Z","shell.execute_reply.started":"2022-10-04T02:32:46.493228Z","shell.execute_reply":"2022-10-04T02:32:46.51683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_imgnet = tmp_df[['img_path', 'cls']]\n# df_imgnet.columns = ['img_path', 'class']","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.525107Z","iopub.execute_input":"2022-10-04T02:32:46.525364Z","iopub.status.idle":"2022-10-04T02:32:46.529647Z","shell.execute_reply.started":"2022-10-04T02:32:46.52534Z","shell.execute_reply":"2022-10-04T02:32:46.528083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_imgnet.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.531283Z","iopub.execute_input":"2022-10-04T02:32:46.531723Z","iopub.status.idle":"2022-10-04T02:32:46.539998Z","shell.execute_reply.started":"2022-10-04T02:32:46.53169Z","shell.execute_reply":"2022-10-04T02:32:46.539084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_imgnet['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.541248Z","iopub.execute_input":"2022-10-04T02:32:46.541817Z","iopub.status.idle":"2022-10-04T02:32:46.551022Z","shell.execute_reply.started":"2022-10-04T02:32:46.541783Z","shell.execute_reply":"2022-10-04T02:32:46.549915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset GUIE","metadata":{}},{"cell_type":"code","source":"df_guie = pd.read_csv('../input/guie-first-dataset-labels/GUIE_FIRST_DATASET.csv')\ndf_guie.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.552527Z","iopub.execute_input":"2022-10-04T02:32:46.553045Z","iopub.status.idle":"2022-10-04T02:32:46.79079Z","shell.execute_reply.started":"2022-10-04T02:32:46.55301Z","shell.execute_reply":"2022-10-04T02:32:46.789779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_guie = df_guie[df_guie[\"supercls\"] != 'toys']\ndf_guie['new_img_path'] = f\"../input/tf-guie-train-from-scratch-part-1/dataset/\" + df_guie['supercls'] + \"/\" + df_guie['cls'] + \"/\" + df_guie['img_path'].str.split('/').str[-1].replace([\"jpg\", \"JPEG\", \"jpeg\", \"JPG\", \"jpe\", \"PNG\", \"bmp\"], \"png\", regex=True)\ndf_guie.to_csv('complete_captions.csv')","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:46.792297Z","iopub.execute_input":"2022-10-04T02:32:46.79291Z","iopub.status.idle":"2022-10-04T02:32:47.73659Z","shell.execute_reply.started":"2022-10-04T02:32:46.792857Z","shell.execute_reply":"2022-10-04T02:32:47.735573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_guie = pd.read_csv(\"/kaggle/working/complete_captions.csv\", usecols=[\"new_img_path\", \"cls\"])\ndf_guie.columns = ['cls', 'image']\ndf_guie['id'] = df_guie.index\ndf_guie = df_guie[~df_guie['id'].isin([49917, 39692, 39691, 39690])] # deleting invalid image\ndf_guie = df_guie[df_guie['image']!=\"../input/tf-guie-train-from-scratch-part-1/dataset/memes/meme_funny_general_not_offensive/image_1567.png\"]\ndf_guie['class'] = df_guie['cls'].str.replace(\"_\", \" \")\ndf_guie = df_guie[['image', 'class']]\ndf_guie.columns = ['img_path', 'class']\n# df_guie.to_csv('guie_labels.csv')\ndf_guie.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:47.738202Z","iopub.execute_input":"2022-10-04T02:32:47.738614Z","iopub.status.idle":"2022-10-04T02:32:47.912487Z","shell.execute_reply.started":"2022-10-04T02:32:47.738572Z","shell.execute_reply":"2022-10-04T02:32:47.91152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_guie['class'].str.replace(\"_\", \" \").value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:47.913883Z","iopub.execute_input":"2022-10-04T02:32:47.91435Z","iopub.status.idle":"2022-10-04T02:32:47.947913Z","shell.execute_reply.started":"2022-10-04T02:32:47.914312Z","shell.execute_reply":"2022-10-04T02:32:47.946891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_guie)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:47.949247Z","iopub.execute_input":"2022-10-04T02:32:47.949674Z","iopub.status.idle":"2022-10-04T02:32:47.95595Z","shell.execute_reply.started":"2022-10-04T02:32:47.949637Z","shell.execute_reply":"2022-10-04T02:32:47.955034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_guie['class'].unique())","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:47.957536Z","iopub.execute_input":"2022-10-04T02:32:47.958198Z","iopub.status.idle":"2022-10-04T02:32:47.972721Z","shell.execute_reply.started":"2022-10-04T02:32:47.958162Z","shell.execute_reply":"2022-10-04T02:32:47.971796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Food","metadata":{}},{"cell_type":"code","source":"import os\n\nimg_path = []\n\ntrain_path = \"../input/food-recognition-2022/raw_data/public_training_set_release_2.0/images/\" \nfor file in os.listdir(train_path):\n    img_path.append(os.path.join(train_path, file))\n    \nval_path = \"../input/food-recognition-2022/raw_data/public_validation_set_2.0/images/\" \nfor file in os.listdir(train_path):\n    img_path.append(os.path.join(train_path, file))","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:47.97402Z","iopub.execute_input":"2022-10-04T02:32:47.974316Z","iopub.status.idle":"2022-10-04T02:32:48.713879Z","shell.execute_reply.started":"2022-10-04T02:32:47.974291Z","shell.execute_reply":"2022-10-04T02:32:48.712891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_food = pd.DataFrame(img_path)\ndf_food = df_food.sample(5000)\nlen(df_food)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:48.715336Z","iopub.execute_input":"2022-10-04T02:32:48.715706Z","iopub.status.idle":"2022-10-04T02:32:48.730244Z","shell.execute_reply.started":"2022-10-04T02:32:48.715663Z","shell.execute_reply":"2022-10-04T02:32:48.729197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_food['class'] = \"food\"\ndf_food.columns = ['img_path', 'class']\ndf_food.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:48.732001Z","iopub.execute_input":"2022-10-04T02:32:48.732715Z","iopub.status.idle":"2022-10-04T02:32:48.745192Z","shell.execute_reply.started":"2022-10-04T02:32:48.732678Z","shell.execute_reply":"2022-10-04T02:32:48.744098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import json\n# from pandas.io.json import json_normalize\n\n# file_path = \"/kaggle/input/food-recognition-2022/raw_data/public_training_set_release_2.0/annotations.json\"\n# with open(file_path) as json_data:\n#     data_train = json.load(json_data)\n\n# file_path = \"/kaggle/input/food-recognition-2022/raw_data/public_validation_set_2.0/annotations.json\"\n# with open(file_path) as json_data:\n#     data_val = json.load(json_data)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:48.746819Z","iopub.execute_input":"2022-10-04T02:32:48.747221Z","iopub.status.idle":"2022-10-04T02:32:48.754599Z","shell.execute_reply.started":"2022-10-04T02:32:48.747185Z","shell.execute_reply":"2022-10-04T02:32:48.753736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_food_train = pd.DataFrame(data_train['categories'])\n# train_path = \"../input/food-recognition-2022/raw_data/public_training_set_release_2.0/images/\" \n# df_food_train[\"img_path\"] = train_path + \"00\"+df_food_train[\"id\"].astype(str)+\".jpg\"\n\n# df_food_val = pd.DataFrame(data_val['categories'])\n# val_path = \"../input/food-recognition-2022/raw_data/public_validation_set_2.0/images/\" \n# df_food_val[\"img_path\"] = val_path + \"00\"+df_food_train[\"id\"].astype(str)+\".jpg\"\n\n# df_food = pd.concat([df_food_train, df_food_val], axis=0)\n# df_food.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:48.757406Z","iopub.execute_input":"2022-10-04T02:32:48.75853Z","iopub.status.idle":"2022-10-04T02:32:48.768126Z","shell.execute_reply.started":"2022-10-04T02:32:48.757659Z","shell.execute_reply":"2022-10-04T02:32:48.767165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_food = df_food[['img_path', 'name_readable']]\n# df_food.columns = ['img_path', 'class']            \n# df_food.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:48.769614Z","iopub.execute_input":"2022-10-04T02:32:48.770244Z","iopub.status.idle":"2022-10-04T02:32:48.778361Z","shell.execute_reply.started":"2022-10-04T02:32:48.770206Z","shell.execute_reply":"2022-10-04T02:32:48.777348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_food.reset_index(inplace=True)\n# df_food['img_path'][0]","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:48.779968Z","iopub.execute_input":"2022-10-04T02:32:48.780407Z","iopub.status.idle":"2022-10-04T02:32:48.789945Z","shell.execute_reply.started":"2022-10-04T02:32:48.780372Z","shell.execute_reply":"2022-10-04T02:32:48.7891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_food['class'] = \"food\"\n# df_food.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:48.793286Z","iopub.execute_input":"2022-10-04T02:32:48.793544Z","iopub.status.idle":"2022-10-04T02:32:48.801034Z","shell.execute_reply.started":"2022-10-04T02:32:48.793518Z","shell.execute_reply":"2022-10-04T02:32:48.799982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apparel","metadata":{}},{"cell_type":"code","source":"import random\n\ndir_path = \"../input/apparel-dataset\"\nfor root, directory, filename in os.walk(dir_path):\n    dirs = directory\n    break\n\ndf_apparel = pd.DataFrame()\nlist_image_path = []\nlist_label = []\nfor _dir in dirs:\n    samples = random.sample((glob(os.path.join(dir_path, _dir) + \"/*.jpg\")), 50)\n    temp = pd.DataFrame({\n        \"img_path\": samples,\n        \"caption\": [(\"apparel, \" + x.split(\"/\")[-2].replace(\"_\", \" \")) for x in samples]\n    })\n    \n    df_apparel = pd.concat([df_apparel, temp])\ndf_apparel.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:48.80279Z","iopub.execute_input":"2022-10-04T02:32:48.803173Z","iopub.status.idle":"2022-10-04T02:32:55.918764Z","shell.execute_reply.started":"2022-10-04T02:32:48.803139Z","shell.execute_reply":"2022-10-04T02:32:55.917822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_apparel.columns = ['img_path', 'class']\ndf_apparel.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:55.920243Z","iopub.execute_input":"2022-10-04T02:32:55.920853Z","iopub.status.idle":"2022-10-04T02:32:55.931064Z","shell.execute_reply.started":"2022-10-04T02:32:55.920814Z","shell.execute_reply":"2022-10-04T02:32:55.930026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Furniture","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nfile_path = \"../input/furniture-images-dataset/furniture_data_img.csv\"\ndf_furniture = pd.read_csv(file_path)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:55.933107Z","iopub.execute_input":"2022-10-04T02:32:55.93352Z","iopub.status.idle":"2022-10-04T02:32:55.977243Z","shell.execute_reply.started":"2022-10-04T02:32:55.933485Z","shell.execute_reply":"2022-10-04T02:32:55.97639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_furniture_mini = df_furniture[df_furniture.Furniture_Type.isin([\n    'Other', 'TV / stereo', 'Storage',\n    'Sofa / living room item', 'Antique / art',\n    'Lighting', 'Textiles / decoration'\n])\n]","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:55.978506Z","iopub.execute_input":"2022-10-04T02:32:55.979223Z","iopub.status.idle":"2022-10-04T02:32:55.985968Z","shell.execute_reply.started":"2022-10-04T02:32:55.979188Z","shell.execute_reply":"2022-10-04T02:32:55.985041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_furniture_mini.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:55.987254Z","iopub.execute_input":"2022-10-04T02:32:55.988107Z","iopub.status.idle":"2022-10-04T02:32:56.004234Z","shell.execute_reply.started":"2022-10-04T02:32:55.988072Z","shell.execute_reply":"2022-10-04T02:32:56.003093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_furniture_mini = df_furniture_mini[['Image_File', 'Furniture_Type']]\ndf_furniture_mini.columns = ['img_path', 'class']\ndf_furniture_mini['img_path'] = \"../input/furniture-images-dataset/furniture_images\" + df_furniture_mini['img_path']\ndf_furniture_mini['class'] = df_furniture_mini['class'].replace(\" /\", \",\", regex=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.006825Z","iopub.execute_input":"2022-10-04T02:32:56.007671Z","iopub.status.idle":"2022-10-04T02:32:56.023199Z","shell.execute_reply.started":"2022-10-04T02:32:56.007636Z","shell.execute_reply":"2022-10-04T02:32:56.022312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_furniture_mini.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.024487Z","iopub.execute_input":"2022-10-04T02:32:56.025079Z","iopub.status.idle":"2022-10-04T02:32:56.039811Z","shell.execute_reply.started":"2022-10-04T02:32:56.025038Z","shell.execute_reply":"2022-10-04T02:32:56.038883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_furniture_mini['img_path'][0]","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.041077Z","iopub.execute_input":"2022-10-04T02:32:56.041434Z","iopub.status.idle":"2022-10-04T02:32:56.04968Z","shell.execute_reply.started":"2022-10-04T02:32:56.041395Z","shell.execute_reply":"2022-10-04T02:32:56.048631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combine Dataset","metadata":{}},{"cell_type":"code","source":"# df_imgnet\n# master_df = pd.concat([df_imgnet, df_guie], ignore_index=True)\n# master_df = pd.concat([master_df, df_laion5b], ignore_index=True)\n\n# master_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.050977Z","iopub.execute_input":"2022-10-04T02:32:56.051462Z","iopub.status.idle":"2022-10-04T02:32:56.059053Z","shell.execute_reply.started":"2022-10-04T02:32:56.05142Z","shell.execute_reply":"2022-10-04T02:32:56.058069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"master_df = pd.concat([\n    df_guie,\n    df_laion5b,\n    df_wit,\n    df_food,\n    df_apparel,\n    df_furniture_mini\n], axis=0)\n\n# master_df.reset_index(inplace=True)\nmaster_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.06236Z","iopub.execute_input":"2022-10-04T02:32:56.062645Z","iopub.status.idle":"2022-10-04T02:32:56.079945Z","shell.execute_reply.started":"2022-10-04T02:32:56.062613Z","shell.execute_reply":"2022-10-04T02:32:56.078957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"master_df = master_df[master_df['class']!='Lighting']\nmaster_df['class'].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.081395Z","iopub.execute_input":"2022-10-04T02:32:56.081809Z","iopub.status.idle":"2022-10-04T02:32:56.113924Z","shell.execute_reply.started":"2022-10-04T02:32:56.081772Z","shell.execute_reply":"2022-10-04T02:32:56.112938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"master_df['encoded_label'] = pd.factorize(master_df['class'])[0]\nmaster_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.115236Z","iopub.execute_input":"2022-10-04T02:32:56.116115Z","iopub.status.idle":"2022-10-04T02:32:56.137132Z","shell.execute_reply.started":"2022-10-04T02:32:56.116077Z","shell.execute_reply":"2022-10-04T02:32:56.136185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(master_df)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.138587Z","iopub.execute_input":"2022-10-04T02:32:56.139385Z","iopub.status.idle":"2022-10-04T02:32:56.145738Z","shell.execute_reply.started":"2022-10-04T02:32:56.139348Z","shell.execute_reply":"2022-10-04T02:32:56.144816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"master_df.encoded_label.max()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.147011Z","iopub.execute_input":"2022-10-04T02:32:56.147954Z","iopub.status.idle":"2022-10-04T02:32:56.15756Z","shell.execute_reply.started":"2022-10-04T02:32:56.147918Z","shell.execute_reply":"2022-10-04T02:32:56.156555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# def make_dataframes(sdir):\n#     bad_images=[]\n#     good_ext=['jpg','jpeg', 'png', 'tiff']# extensions compatible with ImageDataGenerator\n#     filepaths=[]\n#     labels=[]\n#     classes=sorted(os.listdir(sdir))    \n#     for klass in classes:\n#         classpath=os.path.join(sdir, klass)\n#         flist=sorted(os.listdir(classpath))\n#         desc=f'{klass:23s}'\n#         for f in tqdm(flist, ncols=110,desc=desc, unit='file', colour='blue'):\n#             fpath=os.path.join(classpath,f)\n#             fl=f.lower()\n#             index=fl.rfind('.')\n#             ext=fl[index+1:]\n# #             import pdb; pdb.set_trace()\n#             if ext in good_ext:                \n#                 try:  # check for defective image files                  \n#                     img=cv2.imread(fpath)\n#                     shape=img.shape\n#                     filepaths.append(fpath)\n#                     labels.append(klass)\n#                 except:\n#                     bad_images.append(fpath) # defective image file\n#                     print('defective image file: ', fpath)\n#             else:\n#                 bad_images.append(fpath) # not compatible with ImageDataGenerator               \n#     Fseries=pd.Series(filepaths, name='filepaths')\n#     Lseries=pd.Series(labels, name='labels')\n#     df=pd.concat([Fseries, Lseries], axis=1)\n#     df['encoded_label'] = pd.factorize(df.labels)[0]\n#     # split df into a train_df, test_df and a valid_df    \n#     train_df, dummy_df=train_test_split(df, train_size=.8, shuffle=True, random_state=123, stratify=df['labels']) \n#     valid_df, test_df=train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify=dummy_df['labels']) \n#     classes=sorted(train_df['labels'].unique())\n#     class_count=len(classes)\n#     sample_df=train_df.sample(n=50, replace=False)\n#     # calculate the average image height and with\n#     ht=0\n#     wt=0\n#     count=0\n#     for i in range(len(sample_df)):\n#         fpath=sample_df['filepaths'].iloc[i]\n#         try:\n#             img=cv2.imread(fpath)\n#             h=img.shape[0]\n#             w=img.shape[1]\n#             wt +=w\n#             ht +=h\n#             count +=1\n#         except:\n#             pass\n#     have=int(ht/count)\n#     wave=int(wt/count)\n#     aspect_ratio=have/wave\n#     print('number of classes in processed dataset= ', class_count)    \n#     counts=list(train_df['labels'].value_counts())    \n#     print('the maximum files in any class in train_df is ', max(counts), '  the minimum files in any class in train_df is ', min(counts))\n#     print('train_df length: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))  \n#     print('average image height= ', have, '  average image width= ', wave, ' aspect ratio h/w= ', aspect_ratio)\n#     length=len(bad_images)\n#     if length >0 and length <50:\n#         print('Below is a list of ', length, ' defective or incompatible image files')\n#         for f in bad_images:\n#             print(f)\n#     return train_df, test_df, valid_df, classes, class_count\n\n\n# sdir=r'../input/guie-custom-data/images_224' # using 224 X 224 dataset\n# train_df, test_df, valid_df, classes, class_count=make_dataframes(sdir)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.159113Z","iopub.execute_input":"2022-10-04T02:32:56.159728Z","iopub.status.idle":"2022-10-04T02:32:56.167829Z","shell.execute_reply.started":"2022-10-04T02:32:56.159679Z","shell.execute_reply":"2022-10-04T02:32:56.167032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_df.shape)\n# print(test_df.shape)\n# print(valid_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.169158Z","iopub.execute_input":"2022-10-04T02:32:56.170048Z","iopub.status.idle":"2022-10-04T02:32:56.181658Z","shell.execute_reply.started":"2022-10-04T02:32:56.17001Z","shell.execute_reply":"2022-10-04T02:32:56.180754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def trim(df, max_samples, min_samples, column):\n#     df=df.copy()\n#     groups=df.groupby(column)    \n#     trimmed_df = pd.DataFrame(columns = df.columns)\n#     groups=df.groupby(column)\n#     for label in df[column].unique(): \n#         group=groups.get_group(label)\n#         count=len(group)    \n#         if count > max_samples:\n#             sampled_group=group.sample(n=max_samples, random_state=123,axis=0)\n#             trimmed_df=pd.concat([trimmed_df, sampled_group], axis=0)\n#         else:\n#             if count>=min_samples:\n#                 sampled_group=group        \n#                 trimmed_df=pd.concat([trimmed_df, sampled_group], axis=0)\n#     print('after trimming, the maximum samples in any class is now ',max_samples, ' and the minimum samples in any class is ', min_samples)\n#     return trimmed_df\n\n# max_samples=200\n# min_samples=200\n# column='labels'\n# train_df=trim(train_df, max_samples, min_samples, column)    \n# working_dir=r'./'\n# img_size=(224,224)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.18322Z","iopub.execute_input":"2022-10-04T02:32:56.183605Z","iopub.status.idle":"2022-10-04T02:32:56.192417Z","shell.execute_reply.started":"2022-10-04T02:32:56.183553Z","shell.execute_reply":"2022-10-04T02:32:56.191474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_df.shape)\n# print(test_df.shape)\n# print(valid_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.193474Z","iopub.execute_input":"2022-10-04T02:32:56.193743Z","iopub.status.idle":"2022-10-04T02:32:56.20601Z","shell.execute_reply.started":"2022-10-04T02:32:56.193709Z","shell.execute_reply":"2022-10-04T02:32:56.205072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.207936Z","iopub.execute_input":"2022-10-04T02:32:56.208232Z","iopub.status.idle":"2022-10-04T02:32:56.215843Z","shell.execute_reply.started":"2022-10-04T02:32:56.208206Z","shell.execute_reply":"2022-10-04T02:32:56.214919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, dummy_df=train_test_split(master_df, train_size=.8, shuffle=True, random_state=123, stratify=master_df['encoded_label']) \ndummy_df = dummy_df[~dummy_df['encoded_label'].isin(dummy_df['encoded_label'].value_counts()[dummy_df['encoded_label'].value_counts() == 1].keys().tolist())]\nvalid_df, test_df=train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify=dummy_df['encoded_label']) \nclasses=sorted(train_df['encoded_label'].unique())\nclass_count=len(classes)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.217164Z","iopub.execute_input":"2022-10-04T02:32:56.217423Z","iopub.status.idle":"2022-10-04T02:32:56.310642Z","shell.execute_reply.started":"2022-10-04T02:32:56.217399Z","shell.execute_reply":"2022-10-04T02:32:56.309717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_count","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.311927Z","iopub.execute_input":"2022-10-04T02:32:56.312591Z","iopub.status.idle":"2022-10-04T02:32:56.320511Z","shell.execute_reply.started":"2022-10-04T02:32:56.312554Z","shell.execute_reply":"2022-10-04T02:32:56.319327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations\nfrom pylab import rcParams","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:56.322293Z","iopub.execute_input":"2022-10-04T02:32:56.3227Z","iopub.status.idle":"2022-10-04T02:32:57.379113Z","shell.execute_reply.started":"2022-10-04T02:32:56.322666Z","shell.execute_reply":"2022-10-04T02:32:57.378177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_size=336\n\ntransforms_train = albumentations.Compose([\n    albumentations.Resize(image_size, image_size, always_apply=True),\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.RandomBrightnessContrast(p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n    albumentations.HueSaturationValue(p=0.5, hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2),\n    albumentations.ShiftScaleRotate(p=0.5, shift_limit=0.0625, scale_limit=0.2, rotate_limit=20),\n    albumentations.CoarseDropout(p=0.5),\n    albumentations.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], \n                             std=[0.26862954, 0.26130258, 0.27577711], \n                             max_pixel_value=255.0, always_apply=True\n                            )\n])\n\ntransforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size, always_apply=True),\n    albumentations.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], \n                             std=[0.26862954, 0.26130258, 0.27577711], \n                             max_pixel_value=255.0, \n                             always_apply=True\n                            )\n])","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:57.38052Z","iopub.execute_input":"2022-10-04T02:32:57.382445Z","iopub.status.idle":"2022-10-04T02:32:57.393845Z","shell.execute_reply.started":"2022-10-04T02:32:57.382405Z","shell.execute_reply":"2022-10-04T02:32:57.392227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLIPDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, image_filenames, labels, preprocess, transform=None):\n        self.image_filenames = list(image_filenames)\n        self.labels = list(labels)\n        self.preprocess = preprocess\n        self.transforms = transform\n    \n    def __len__(self):\n        return len(self.labels)\n        \n    \n    # Get a Sample     \n    def __getitem__(self, idx):\n        item = {}\n        \n        if self.transforms is not None:\n\n            image = cv2.imread(f\"{self.image_filenames[idx]}\")\n            try:\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            except:\n                print(f\"{self.image_filenames[idx]}\")\n                print(f\"{self.captions[idx]}\")\n                print(idx)\n                \n            image = self.transforms(image=image)['image']\n            item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n            \n#             res = self.transform(image=self.preprocess(Image.open(self.image_filenames[idx])))\n#             img = res['image']\n#             img = img.astype(np.float32)\n#             item['image'] = img.transpose(2,0,1)\n\n        else:   \n            item['image'] = self.preprocess(Image.open(self.image_filenames[idx]))\n                                 \n        item['label'] = self.labels[idx]\n\n        return item\n    ","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:57.395412Z","iopub.execute_input":"2022-10-04T02:32:57.395919Z","iopub.status.idle":"2022-10-04T02:32:57.409322Z","shell.execute_reply.started":"2022-10-04T02:32:57.395882Z","shell.execute_reply":"2022-10-04T02:32:57.408282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = CLIPDataset(\n    train_df['img_path'].values, \n    train_df['encoded_label'].values, \n    preprocess=preprocess, \n    transform = transforms_train\n)\nrcParams['figure.figsize'] = 15,5\nfor i in range(2):\n    f, axarr = plt.subplots(1,5)\n    for p in range(5):\n        idx = i*5 + p\n        item = dataset[idx]\n        img, label = item['image'], item['label']\n        axarr[p].imshow(img.transpose(0,1).transpose(1,2).squeeze())\n        axarr[p].set_title(label.item())","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:57.41095Z","iopub.execute_input":"2022-10-04T02:32:57.411425Z","iopub.status.idle":"2022-10-04T02:32:59.212594Z","shell.execute_reply.started":"2022-10-04T02:32:57.411391Z","shell.execute_reply":"2022-10-04T02:32:59.211692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_loaders(dataframe, mode):\n    \n    dataset = CLIPDataset(\n        dataframe[\"img_path\"].values,\n        dataframe[\"encoded_label\"].values,\n        preprocess=preprocess,\n        transform = transforms_train if mode == \"train\" else transforms_valid\n    )\n    \n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=BATCH_SIZE,\n        num_workers=WORKERS,\n        shuffle=True if mode == \"train\" else False,\n    )\n    \n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:59.214265Z","iopub.execute_input":"2022-10-04T02:32:59.214987Z","iopub.status.idle":"2022-10-04T02:32:59.22191Z","shell.execute_reply.started":"2022-10-04T02:32:59.214946Z","shell.execute_reply":"2022-10-04T02:32:59.220679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- Normalize the embeddings and weights\nCalculate the dot products\nCalculate the angles with arccos\nAdd a constant factor m to the angle corresponding to the ground truth label\nTurn angles back to cosines\nUse cross entropy on the new cosine values to calculate loss -->","metadata":{}},{"cell_type":"markdown","source":"# ArcFace implementation","metadata":{}},{"cell_type":"markdown","source":"**ArcFace Calculation:**\n\n* Normalize the embeddings and weights\n* Calculate the dot products\n* Calculate the angles with arccos\n* Add a constant factor m to the angle corresponding to the ground truth label\n* Turn angles back to cosines\n* Use cross entropy on the new cosine values to calculate loss","metadata":{}},{"cell_type":"code","source":"# FIRST ATTEMPT\n\n# class ArcFaceClassifier(nn.Module):\n#     def __init__(self, emb_size, output_classes):\n#         super().__init__()\n#         self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n#         nn.init.kaiming_uniform_(self.W)\n#     def forward(self, x):\n#         # Step 1: Normalize the embeddings and weights\n#         x_norm = F.normalize(x).float()\n#         W_norm = F.normalize(self.W, dim=0).to(device)\n#         # Step 2: Calculate the dot products\n#         return x_norm @ W_norm\n    \n# def arcface_loss(cosine, targ, m=.4):\n#     # this prevents nan when a value slightly crosses 1.0 due to numerical error\n#     cosine = cosine.clip(-1+1e-7, 1-1e-7) \n#     # Step 3: Calculate the angles with arccos\n#     arcosine = cosine.arccos()\n#     # Step 4: Add a constant factor m to the angle corresponding to the ground truth label\n#     arcosine += F.one_hot(targ, num_classes = OUTPUT_CLASSES) * m\n#     # Step 5: Turn angles back to cosines\n#     cosine2 = arcosine.cos()\n#     # Step 6: Use cross entropy on the new cosine values to calculate loss\n#     return F.cross_entropy(cosine2, targ)\n\n# # Not using for now\n# def cross_entropy(preds, targets, reduction='none'):\n#     log_softmax = nn.LogSoftmax(dim=-1)\n#     loss = (-targets * log_softmax(preds)).sum(1)\n#     if reduction == \"none\":\n#         return loss\n#     elif reduction == \"mean\":\n#         return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:59.223798Z","iopub.execute_input":"2022-10-04T02:32:59.22458Z","iopub.status.idle":"2022-10-04T02:32:59.238606Z","shell.execute_reply.started":"2022-10-04T02:32:59.224543Z","shell.execute_reply":"2022-10-04T02:32:59.237327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcModule(nn.Module):\n    def __init__(self, in_features, out_features, s = 30, m = MARGIN):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n#         nn.init.xavier_uniform_(self.weight)  # TRY\n        nn.init.xavier_normal_(self.weight)\n\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = torch.tensor(math.cos(math.pi - m))\n        self.mm = torch.tensor(math.sin(math.pi - m) * m)\n\n    def forward(self, inputs, labels):\n#         import pdb; pdb.set_trace()\n        cos_th = F.linear(inputs, F.normalize(self.weight))\n#         print(cos_th)\n        cos_th = cos_th.clamp(-1, 1)\n        sin_th = torch.sqrt(1.0 - torch.pow(cos_th, 2))\n#         sin_th = torch.sqrt(torch.clamp((1.0 - torch.pow(cos_th, 2)),1e-9,1))\n        cos_th_m = cos_th * self.cos_m - sin_th * self.sin_m\n        # print(type(cos_th), type(self.th), type(cos_th_m), type(self.mm))\n        cos_th_m = torch.where(cos_th > self.th, cos_th_m, cos_th - self.mm)\n\n        cond_v = cos_th - self.th\n        cond = cond_v <= 0\n        cos_th_m[cond] = (cos_th - self.mm)[cond]\n        \n        if labels.dim() == 1:\n            labels = labels.unsqueeze(-1)\n        onehot = torch.zeros(cos_th.size()).cuda()\n        labels = labels.type(torch.LongTensor).cuda()\n        onehot.scatter_(1, labels, 1.0)\n#         onehot = torch.squeeze(F.one_hot(labels, num_classes = 9)).float()\n        outputs = onehot * cos_th_m + (1.0 - onehot) * cos_th\n        outputs = outputs * self.s\n#         print(outputs)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:59.240021Z","iopub.execute_input":"2022-10-04T02:32:59.240461Z","iopub.status.idle":"2022-10-04T02:32:59.253957Z","shell.execute_reply.started":"2022-10-04T02:32:59.240424Z","shell.execute_reply":"2022-10-04T02:32:59.252971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class LambdaLayer(nn.Module):\n#     def __init__(self, lambd):\n#         super(LambdaLayer, self).__init__()\n#         self.lambd = lambd\n#     def forward(self, x):\n#         return self.lambd(x)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:59.256506Z","iopub.execute_input":"2022-10-04T02:32:59.256765Z","iopub.status.idle":"2022-10-04T02:32:59.267805Z","shell.execute_reply.started":"2022-10-04T02:32:59.256742Z","shell.execute_reply":"2022-10-04T02:32:59.266838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from torch import nn\n# x=torch.tensor([2])\n# Lambda = LambdaLayer(lambda x: x )\n# print(Lambda(x))","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:59.269266Z","iopub.execute_input":"2022-10-04T02:32:59.269554Z","iopub.status.idle":"2022-10-04T02:32:59.278931Z","shell.execute_reply.started":"2022-10-04T02:32:59.269514Z","shell.execute_reply":"2022-10-04T02:32:59.27799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FIRST ATTEMPT\n\n# backbone(CLIP) + Dropout + Dense(units=256) + Arcface + Softmax (classes=17691)\n# class MyModel(nn.Module):\n#     def __init__(self, embedding_dim, output_classes):\n#         super(MyModel, self).__init__()\n#         self.embedding_dim = embedding_dim\n#         self.output_classes = output_classes\n#         self.lambd = LambdaLayer(lambda x: x)\n#         self.model, self.preprocess = clip.load(\"ViT-L/14\", device=device, jit=True) # Do I have to use just Visual model??\n#         self.w_dropout = nn.Dropout(p=0.2)\n#         self.w_linear1 = nn.Linear(768, self.embedding_dim, dtype=torch.float16, device = device)\n#         self.w_arcfaceclassifier = ArcFaceClassifier(self.embedding_dim, self.output_classes)\n#         self.w_adaptive_avg_pool = nn.AdaptiveAvgPool1d(64)\n#         self.w_softmax = nn.Softmax()\n        \n#     def get_embs(self, x):\n#         img_embeddings = self.model.encode_image(x.half())\n#         l = self.w_dropout(img_embeddings)\n#         l = self.w_linear1(l)\n# #         l = normalize(l, dim = -1, p = 2.0)\n#         return l\n    \n#     def forward(self, x):\n#         x = self.get_embs(x)\n#         x = self.w_arcfaceclassifier(x)\n        \n# #         x = self.softmax(x)\n# #         x = normalize(x, dim = -1, p = 2.0)\n        \n        \n# #         if self.embedding_dim != 64:\n# #             x = self.adaptive_avg_pool(x)\n# #         else:\n# #             x = self.lambd(lambda x: x )\n            \n#         return x","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:59.280257Z","iopub.execute_input":"2022-10-04T02:32:59.28062Z","iopub.status.idle":"2022-10-04T02:32:59.29004Z","shell.execute_reply.started":"2022-10-04T02:32:59.280593Z","shell.execute_reply":"2022-10-04T02:32:59.289188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageEncoder(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n#         self.model, self.preprocess = clip.load(\"ViT-L/14\", device=device, jit=True)\n        self.model, self.preprocess = clip.load(\"ViT-L/14@336px\", jit=True, device=device)\n        \n    def forward(self, mini_batch_images):\n\n        img_emb = self.model.encode_image(mini_batch_images.half())\n\n        return img_emb","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:32:59.291724Z","iopub.execute_input":"2022-10-04T02:32:59.292099Z","iopub.status.idle":"2022-10-04T02:32:59.304578Z","shell.execute_reply.started":"2022-10-04T02:32:59.292065Z","shell.execute_reply":"2022-10-04T02:32:59.303584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class GeM(nn.Module):\n#     def __init__(self, kernel_size=2, p=3, eps=1e-6):\n#         super(GeM, self).__init__()\n#         self.p = nn.Parameter(torch.tensor(torch.ones(1)*p, dtype=torch.float16, device = device)) #.to(device) #nn.Parameter(torch.ones(1)*p)\n#         self.kernel_size = kernel_size\n#         self.eps = eps\n\n#     def forward(self, x):\n#         return self.gem(x, p=self.p, eps=self.eps)\n        \n#     def gem(self, x, p=3, eps=1e-6):\n#         return F.avg_pool1d(x.clamp(min=eps).pow(p), self.kernel_size).pow(1./p)\n# #         return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n        \n#     def __repr__(self):\n#         return self.__class__.__name__ + \\\n#                 '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n#                 ', ' + 'eps=' + str(self.eps) + ')'\n\nclass GeM(nn.Module):\n    def __init__(self, kernel_size=2, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.FloatTensor(torch.ones(1)*p)) # nn.Parameter(torch.FloatTensor(out_features, in_features))\n        self.kernel_size = kernel_size\n        self.eps = eps\n\n#     def forward(self, x):\n#         return self.gem(x, p=self.p, eps=self.eps)\n        \n#     def gem(self, x, p=3, eps=1e-6):\n#         return F.avg_pool1d(x.clamp(min=eps).pow(p), self.kernel_size).pow(1./p)\n#         return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n\n    def forward(self, x):\n        return self.gem(x)\n\n    def gem(self, x):\n        return F.avg_pool1d(x.clamp(min=1e-6).pow(3), self.kernel_size).pow(1./3)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + \\\n                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n                ', ' + 'eps=' + str(self.eps) + ')'","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:31:43.427599Z","iopub.execute_input":"2022-10-04T06:31:43.428031Z","iopub.status.idle":"2022-10-04T06:31:43.437922Z","shell.execute_reply.started":"2022-10-04T06:31:43.427995Z","shell.execute_reply":"2022-10-04T06:31:43.436937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ProjectionHead(nn.Module):\n    def __init__(\n        self,\n        embedding_dim=EMB_SIZE,\n        output_classes=OUTPUT_CLASSES, \n        dropout=0.2\n    ):\n        super().__init__()\n        \n        self.embedding_dim = embedding_dim\n        self.output_classes = output_classes\n        self.clip_embedding_dim = 384 #192 #768\n        \n        self.w_pooling = GeM()\n        self.w_dropout = nn.Dropout(p=dropout, inplace=True)\n        self.w_fc1 = nn.Linear(self.clip_embedding_dim, self.embedding_dim, device = device) #dtype=torch.float16, \n        self.bn1 = nn.BatchNorm1d(384) #192) #768)\n        self.bn2 = nn.BatchNorm1d(64)\n    \n    def forward(self, x):\n        \n        pooled_features = self.w_pooling(x.float())\n        features = self.bn1(pooled_features)\n        features = self.w_dropout(features)\n        features = self.w_fc1(features)\n        features = self.bn2(features)\n        features = F.normalize(features, dim = -1, p = 2.0)\n        \n        return features","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:31:43.88292Z","iopub.execute_input":"2022-10-04T06:31:43.883323Z","iopub.status.idle":"2022-10-04T06:31:43.89365Z","shell.execute_reply.started":"2022-10-04T06:31:43.883289Z","shell.execute_reply":"2022-10-04T06:31:43.892529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcFaceMargin(nn.Module):\n    def __init__(\n        self,\n        embedding_dim=EMB_SIZE,\n        output_classes=OUTPUT_CLASSES\n    ):\n        super().__init__()\n        \n        self.embedding_dim = embedding_dim\n        self.output_classes = output_classes\n        self.warcface_margin = ArcModule(in_features=self.embedding_dim, out_features = self.output_classes)\n    \n    def forward(self, features, labels):\n        logits = self.warcface_margin(features, labels) #features.float()\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:31:44.230323Z","iopub.execute_input":"2022-10-04T06:31:44.230704Z","iopub.status.idle":"2022-10-04T06:31:44.238007Z","shell.execute_reply.started":"2022-10-04T06:31:44.23067Z","shell.execute_reply":"2022-10-04T06:31:44.236723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        \n        self.image_encoder = ImageEncoder()\n        self.image_projection = ProjectionHead()\n\n        self.warcface_margin = ArcFaceMargin()\n        \n    \n    def forward(self, x, labels=None):\n        \n        pretrained_clip_features = self.image_encoder(x)\n        features = self.image_projection(pretrained_clip_features)\n\n        if labels is not None:\n            features = self.warcface_margin(features, labels)\n        \n        return features\n","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:31:44.549856Z","iopub.execute_input":"2022-10-04T06:31:44.550799Z","iopub.status.idle":"2022-10-04T06:31:44.558148Z","shell.execute_reply.started":"2022-10-04T06:31:44.550756Z","shell.execute_reply":"2022-10-04T06:31:44.557042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class MyModel(nn.Module):\n#     def __init__(self, embedding_dim, output_classes):\n#         super(MyModel, self).__init__()\n#         self.embedding_dim = embedding_dim\n#         self.output_classes = output_classes\n#         self.clip_embedding_dim = 768\n#         self.image_encoder = ImageEncoder()\n# #         self.model, self.preprocess = clip.load(\"ViT-L/14\", device=device, jit=True)\n# #         self.model = self.model.visual # Do I have to use just Visual model??\n#         self.w_dropout = nn.Dropout(p=0.2, inplace=True)\n# #         self.bn1 = nn.BatchNorm2d(self.clip_embedding_dim)\n#         self.w_fc1 = nn.Linear(self.clip_embedding_dim, self.embedding_dim, dtype=torch.float16, device = device) #self.embedding_dim\n# #         self.bn2 = nn.BatchNorm1d(self.embedding_dim)\n#         self.warcface_margin = ArcModule(in_features=self.embedding_dim, out_features = self.output_classes)\n\n#     def clip_embedding(self, x):\n#         features = self.image_encoder(x)\n#         return features\n    \n#     def get_embedding(self, x):\n        \n# #         features = self.bn1(features)\n#         features = self.w_dropout(x)\n# #         features = features.view(features.size(0), -1)\n# #         print(features.shape)\n#         features = self.w_fc1(features)\n# #         features = self.bn2(features)\n#         features = F.normalize(features, dim = -1, p = 2.0) # normalize(img_emb, dim = -1, p = 2.0)\n#         return features\n    \n#     def compute_arcface(self, features, labels=None):\n#         if labels is not None:\n#             return self.warcface_margin(features.float(), labels)\n    \n#     def forward(self, x, labels=None):\n        \n#         clip_features = self.clip_embedding(x)\n#         features = self.get_embedding(clip_features)\n        \n#         if labels is not None:\n#             features = self.compute_arcface(features, labels)\n        \n#         return features\n","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:31:45.300822Z","iopub.execute_input":"2022-10-04T06:31:45.301568Z","iopub.status.idle":"2022-10-04T06:31:45.307681Z","shell.execute_reply.started":"2022-10-04T06:31:45.30153Z","shell.execute_reply":"2022-10-04T06:31:45.306443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = MyModel()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:31:45.636953Z","iopub.execute_input":"2022-10-04T06:31:45.638009Z","iopub.status.idle":"2022-10-04T06:31:45.642467Z","shell.execute_reply.started":"2022-10-04T06:31:45.637966Z","shell.execute_reply":"2022-10-04T06:31:45.641485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for params in model.parameters():\n#     params.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:31:45.933345Z","iopub.execute_input":"2022-10-04T06:31:45.93367Z","iopub.status.idle":"2022-10-04T06:31:45.938423Z","shell.execute_reply.started":"2022-10-04T06:31:45.933641Z","shell.execute_reply":"2022-10-04T06:31:45.937295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for name, param in model.named_parameters():\n#     if 'w_' in name or 'warcface_' in name:\n#         print(name)\n#         param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:31:46.242545Z","iopub.execute_input":"2022-10-04T06:31:46.242911Z","iopub.status.idle":"2022-10-04T06:31:46.24789Z","shell.execute_reply.started":"2022-10-04T06:31:46.242858Z","shell.execute_reply":"2022-10-04T06:31:46.246641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for params in model.parameters():\n#     if params.requires_grad:\n#         print(params.shape)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:31:46.58852Z","iopub.execute_input":"2022-10-04T06:31:46.588843Z","iopub.status.idle":"2022-10-04T06:31:46.59298Z","shell.execute_reply.started":"2022-10-04T06:31:46.588814Z","shell.execute_reply":"2022-10-04T06:31:46.592035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"class AvgMeter:\n    def __init__(self, name=\"Metric\"):\n        self.name = name\n        self.reset()\n\n    def reset(self):\n        self.avg, self.sum, self.count = [0] * 3\n\n    def update(self, val, count=1):\n        self.count += count\n        self.sum += val * count\n        self.avg = self.sum / self.count\n\n    def __repr__(self):\n        text = f\"{self.name}: {self.avg:.4f}\"\n        return text\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[\"lr\"]\n","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:31:47.602803Z","iopub.execute_input":"2022-10-04T06:31:47.603525Z","iopub.status.idle":"2022-10-04T06:31:47.610607Z","shell.execute_reply.started":"2022-10-04T06:31:47.603486Z","shell.execute_reply":"2022-10-04T06:31:47.609507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, train_loader, optimizer, criterion):\n\n    loss_meter = AvgMeter()\n    tqdm_object = tqdm(train_loader, total=len(train_loader))\n    \n    losses = []\n    for batch in tqdm_object:\n        \n        batch = {k: v.to(device) for k, v in batch.items()}\n#         import pdb; pdb.set_trace()\n        logits = model(batch['image'], batch['label'])\n#         print(logits.shape)\n#         print(batch['label'].max())\n        loss = criterion(logits, batch['label'])\n#         print(logits)\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        count = batch[\"image\"].size(0)\n        loss_meter.update(loss.item(), count)\n        \n        losses.append(loss.item())\n        \n        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n    \n    loss_train = np.mean(losses)\n    print(f\"Epoch loss:{loss_train}\")\n    return loss_meter, loss_train\n\n\ndef valid_epoch(model, valid_loader, criterion):\n\n    loss_meter = AvgMeter()\n    \n    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n    \n    TARGETS = []\n    losses = []\n    PREDS = []\n    \n    for batch in tqdm_object:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        logits = model(batch['image'], batch['label'].to(device).long())\n        loss = criterion(logits, batch['label'].to(device).long())\n\n        PREDS += [torch.argmax(logits, 1).detach().cpu()]\n        TARGETS += [batch['label'].to(device).long().detach().cpu()]\n            \n        count = batch[\"image\"].size(0)\n        loss_meter.update(loss.item(), count)\n        \n        losses.append(loss.item())\n\n        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n        \n    PREDS = torch.cat(PREDS).cpu().numpy()\n    TARGETS = torch.cat(TARGETS).cpu().numpy()\n    accuracy = (PREDS==TARGETS).mean()\n   \n    loss_valid = np.mean(losses)\n    \n    return loss_meter, loss_valid, accuracy\n\n\ndef main():\n\n    train_loader = build_loaders(train_df, mode=\"train\")\n    valid_loader = build_loaders(valid_df, mode=\"valid\")\n\n    model = MyModel().to(device)  \n    \n    for params in model.parameters():\n        params.requires_grad = False\n    \n    for name, param in model.named_parameters():\n        if 'w_' in name or 'warcface_' in name:\n            param.requires_grad = True\n    \n    print(\"Trainable paramaters :\")\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            print(name)\n\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n                           lr=1e-3,betas=(0.9,0.98),\n                           eps=1e-6,\n                           weight_decay=0.2)\n\n#     optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n#                            lr=1e-3, weight_decay=0.2)\n    \n    criterion = nn.CrossEntropyLoss()\n    scheduler = CosineAnnealingLR(optimizer,T_max=2, \n                                                   eta_min=1e-4)\n\n#     PATH = \"/kaggle/input/arcface-10epochs-checkpoint-0406/best.pt\" \n# #     PATH = \"/kaggle/input/arcface-15moreepochs-checkpoint/best.pt\" \n#     PATH = \"/kaggle/working/best.pt\" \n#     checkpoint = torch.load(PATH)\n#     model.load_state_dict(checkpoint)\n    \n    best_loss = float('inf')\n    for epoch in range(EPOCH):\n        \n        print(f\"Epoch: {epoch + 1}\")\n\n        model.train()\n        train_loss, train_loss_val = train_epoch(model, train_loader, optimizer, criterion)\n        \n        model.eval()\n        with torch.no_grad():\n            valid_loss, valid_loss_val, accuracy = valid_epoch(model, valid_loader, criterion)\n        \n        TRAIN_LOSS.append(train_loss)\n        VAL_LOSS.append(valid_loss)\n        \n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            print(f\"loss : {best_loss}\")\n            torch.save(model.state_dict(), \"best.pt\")\n            print(\"Saved Best Model!\")\n        \n        print(f\"Train Loss : {train_loss}; Validation Loss : {valid_loss}; Best Loss : {best_loss}; Val Accuracy : {accuracy}\")\n        scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:31:48.218832Z","iopub.execute_input":"2022-10-04T06:31:48.219373Z","iopub.status.idle":"2022-10-04T06:31:48.239098Z","shell.execute_reply.started":"2022-10-04T06:31:48.219337Z","shell.execute_reply":"2022-10-04T06:31:48.238121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train_epoch(model, train_loader, optimizer, criterion):\n\n#     loss_meter = AvgMeter()\n#     tqdm_object = tqdm(train_loader, total=len(train_loader))\n    \n#     losses = []\n#     for batch in tqdm_object:\n        \n#         batch = {k: v.to(device) for k, v in batch.items()}\n#         logits = model(batch['image'], batch['label'])\n#         loss = criterion(logits, batch['label'])\n# #         print(logits)\n# #         optimizer.zero_grad()\n# #         loss.backward()\n# #         optimizer.step()\n#         loss.backward()\n#         optimizer.step()\n#         optimizer.zero_grad()\n        \n#         count = batch[\"image\"].size(0)\n#         loss_meter.update(loss.item(), count)\n        \n#         losses.append(loss.item())\n        \n#         tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n    \n#     loss_train = np.mean(losses)\n#     print(f\"Epoch loss:{loss_train}\")\n#     return loss_meter, loss_train\n\n\n# def valid_epoch(model, valid_loader, criterion):\n\n#     loss_meter = AvgMeter()\n    \n#     tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n    \n#     TARGETS = []\n#     losses = []\n#     PREDS = []\n    \n#     for batch in tqdm_object:\n#         batch = {k: v.to(device) for k, v in batch.items()}\n        \n#         logits = model(batch['image'], batch['label'].to(device).long())\n#         loss = criterion(logits, batch['label'].to(device).long())\n\n#         PREDS += [torch.argmax(logits, 1).detach().cpu()]\n#         TARGETS += [batch['label'].to(device).long().detach().cpu()]\n            \n#         count = batch[\"image\"].size(0)\n#         loss_meter.update(loss.item(), count)\n        \n#         losses.append(loss.item())\n\n#         tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n        \n#     PREDS = torch.cat(PREDS).cpu().numpy()\n#     TARGETS = torch.cat(TARGETS).cpu().numpy()\n#     accuracy = (PREDS==TARGETS).mean()\n   \n#     loss_valid = np.mean(losses)\n    \n#     return loss_meter, loss_valid, accuracy\n\n\n# def main():\n\n#     train_loader = build_loaders(train_df, mode=\"train\")\n#     valid_loader = build_loaders(valid_df, mode=\"valid\")\n\n#     model = MyModel(EMB_SIZE, OUTPUT_CLASSES).to(device)  \n    \n#     for params in model.parameters():\n#         params.requires_grad = False\n    \n#     for name, param in model.named_parameters():\n#         if 'w_' in name:\n#             param.requires_grad = True\n    \n#     print(\"Trainable paramaters :\")\n#     for name, param in model.named_parameters():\n#         if param.requires_grad:\n#             print(name)\n\n#     optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n#                            lr=1e-3,betas=(0.9,0.98),\n#                            eps=1e-6,\n#                            weight_decay=0.2)\n    \n#     criterion = nn.CrossEntropyLoss()\n# #     optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n# #     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCH)\n\n#     best_loss = float('inf')\n#     for epoch in range(EPOCH):\n        \n#         print(f\"Epoch: {epoch + 1}\")\n# #         scheduler.step()\n#         model.train()\n#         train_loss, train_loss_val = train_epoch(model, train_loader, optimizer, criterion)\n        \n#         model.eval()\n#         with torch.no_grad():\n#             valid_loss, valid_loss_val, accuracy = valid_epoch(model, valid_loader, criterion)\n        \n#         TRAIN_LOSS.append(train_loss)\n#         VAL_LOSS.append(valid_loss)\n        \n#         if valid_loss.avg < best_loss:\n#             best_loss = valid_loss.avg\n#             print(f\"loss : {best_loss}\")\n#             torch.save(model.state_dict(), \"best.pt\")\n#             print(\"Saved Best Model!\")\n        \n#         print(f\"Train Loss : {train_loss}; Validation Loss : {valid_loss}; Best Loss : {best_loss}; Val Accuracy : {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-04T04:19:16.614291Z","iopub.execute_input":"2022-10-04T04:19:16.614601Z","iopub.status.idle":"2022-10-04T04:19:16.621774Z","shell.execute_reply.started":"2022-10-04T04:19:16.614573Z","shell.execute_reply":"2022-10-04T04:19:16.620501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T04:19:17.338903Z","iopub.execute_input":"2022-10-04T04:19:17.340238Z","iopub.status.idle":"2022-10-04T05:50:41.71061Z","shell.execute_reply.started":"2022-10-04T04:19:17.340182Z","shell.execute_reply":"2022-10-04T05:50:41.709185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T02:36:23.351427Z","iopub.execute_input":"2022-10-04T02:36:23.35182Z","iopub.status.idle":"2022-10-04T04:07:46.828698Z","shell.execute_reply.started":"2022-10-04T02:36:23.351775Z","shell.execute_reply":"2022-10-04T04:07:46.82758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T12:55:00.119082Z","iopub.execute_input":"2022-10-03T12:55:00.121431Z","iopub.status.idle":"2022-10-03T14:29:01.828941Z","shell.execute_reply.started":"2022-10-03T12:55:00.121394Z","shell.execute_reply":"2022-10-03T14:29:01.827507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main() # Data Augmentation","metadata":{"execution":{"iopub.status.busy":"2022-10-03T08:39:06.048881Z","iopub.execute_input":"2022-10-03T08:39:06.04926Z","iopub.status.idle":"2022-10-03T10:12:19.885266Z","shell.execute_reply.started":"2022-10-03T08:39:06.049226Z","shell.execute_reply":"2022-10-03T10:12:19.883967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T04:09:32.352042Z","iopub.execute_input":"2022-10-03T04:09:32.352399Z","iopub.status.idle":"2022-10-03T07:15:32.036324Z","shell.execute_reply.started":"2022-10-03T04:09:32.352368Z","shell.execute_reply":"2022-10-03T07:15:32.035028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main()","metadata":{"execution":{"iopub.status.busy":"2022-10-02T19:12:20.1884Z","iopub.execute_input":"2022-10-02T19:12:20.18935Z","iopub.status.idle":"2022-10-03T01:10:40.694375Z","shell.execute_reply.started":"2022-10-02T19:12:20.189311Z","shell.execute_reply":"2022-10-03T01:10:40.691822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T01:10:40.696972Z","iopub.status.idle":"2022-10-03T01:10:40.698379Z","shell.execute_reply.started":"2022-10-03T01:10:40.697982Z","shell.execute_reply":"2022-10-03T01:10:40.69802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Image.open('../input/food-recognition-2022/raw_data/public_training_set_release_2.0/images/006316.jpg')","metadata":{"execution":{"iopub.status.busy":"2022-10-03T01:10:40.700673Z","iopub.status.idle":"2022-10-03T01:10:40.70184Z","shell.execute_reply.started":"2022-10-03T01:10:40.701486Z","shell.execute_reply":"2022-10-03T01:10:40.70152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T01:10:40.703971Z","iopub.status.idle":"2022-10-03T01:10:40.705019Z","shell.execute_reply.started":"2022-10-03T01:10:40.70468Z","shell.execute_reply":"2022-10-03T01:10:40.704711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T01:10:40.707106Z","iopub.status.idle":"2022-10-03T01:10:40.708373Z","shell.execute_reply.started":"2022-10-03T01:10:40.707954Z","shell.execute_reply":"2022-10-03T01:10:40.707989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T01:10:40.710555Z","iopub.status.idle":"2022-10-03T01:10:40.711755Z","shell.execute_reply.started":"2022-10-03T01:10:40.711364Z","shell.execute_reply":"2022-10-03T01:10:40.711397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8,5))\nax = fig.add_subplot(1,1,1)\nax.plot([float(str(i).split(\": \")[-1]) for i in TRAIN_LOSS], label='Train')\nax.plot([float(str(i).split(\": \")[-1]) for i in VAL_LOSS], label='Valid')\n\nax.set_title(\"Tain Vs Validation Loss\")\nax.set_xlabel(\"Epoch\")\nax.set_ylabel(\"Loss\")\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T10:14:48.67929Z","iopub.execute_input":"2022-10-03T10:14:48.679704Z","iopub.status.idle":"2022-10-03T10:14:48.910223Z","shell.execute_reply.started":"2022-10-03T10:14:48.679669Z","shell.execute_reply":"2022-10-03T10:14:48.909116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Try model on Test Set","metadata":{}},{"cell_type":"code","source":"def get_image_embeddings(valid_df, model_path):\n\n    test_loader = build_loaders(test_df, mode=\"test\")\n    \n    model = MyModel().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.eval()\n    \n    test_image_embeddings = []\n    losses = []\n    PREDS = []\n    TARGETS = []\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader):\n            logits = model(batch['image'], batch['label'].to(device).long())\n            loss = criterion(logits, batch['label'].to(device).long())\n            losses.append(loss)\n            \n            PREDS += [torch.argmax(logits, 1).detach().cpu()]\n            TARGETS += [batch['label'].to(device).long().detach().cpu()]\n        \n        test_loss = np.mean(torch.tensor(losses).cpu().numpy())\n        PREDS = torch.cat(PREDS).cpu().numpy()\n        TARGETS = torch.cat(TARGETS).cpu().numpy()\n        accuracy = (PREDS==TARGETS).mean()\n#             image_embeddings = model(batch[\"image\"].to(device)).float()\n#             test_image_embeddings.append(image_embeddings)\n    return model, test_loss, accuracy #torch.cat(test_image_embeddings)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T01:10:54.985832Z","iopub.execute_input":"2022-10-03T01:10:54.986271Z","iopub.status.idle":"2022-10-03T01:10:55.000215Z","shell.execute_reply.started":"2022-10-03T01:10:54.986241Z","shell.execute_reply":"2022-10-03T01:10:54.998648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model, image_embeddings = get_image_embeddings(test_df, \"best.pt\")\nmodel, loss, accuracy = get_image_embeddings(test_df, \"best.pt\")\nprint(f\"test loss: {loss}; test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-03T01:10:55.877519Z","iopub.execute_input":"2022-10-03T01:10:55.878717Z","iopub.status.idle":"2022-10-03T01:30:55.984463Z","shell.execute_reply.started":"2022-10-03T01:10:55.878654Z","shell.execute_reply":"2022-10-03T01:30:55.982438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model submission","metadata":{}},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:31:52.649448Z","iopub.execute_input":"2022-10-04T06:31:52.649914Z","iopub.status.idle":"2022-10-04T06:31:52.660308Z","shell.execute_reply.started":"2022-10-04T06:31:52.649838Z","shell.execute_reply":"2022-10-04T06:31:52.659418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MyModel().to(device)\nmodel.load_state_dict(torch.load(\"best.pt\", map_location=device))\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:31:52.985817Z","iopub.execute_input":"2022-10-04T06:31:52.986505Z","iopub.status.idle":"2022-10-04T06:32:00.051682Z","shell.execute_reply.started":"2022-10-04T06:31:52.986464Z","shell.execute_reply":"2022-10-04T06:32:00.050648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureEmbeddingModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.clip_feature_extractor = model.image_encoder\n        self.projection = model.image_projection\n            \n    def forward(self, x):\n        x = transforms.functional.resize(x,size=[336, 336]) # resize(x,size=[224, 224])\n        x = x/255.0\n        x = transforms.functional.normalize(x, mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n        x = self.clip_feature_extractor(x)\n        x = self.projection(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:32:00.053688Z","iopub.execute_input":"2022-10-04T06:32:00.054336Z","iopub.status.idle":"2022-10-04T06:32:00.061609Z","shell.execute_reply.started":"2022-10-04T06:32:00.054295Z","shell.execute_reply":"2022-10-04T06:32:00.060625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embd_model = FeatureEmbeddingModel().to(device)\nembd_model","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:32:00.063116Z","iopub.execute_input":"2022-10-04T06:32:00.063472Z","iopub.status.idle":"2022-10-04T06:32:00.08865Z","shell.execute_reply.started":"2022-10-04T06:32:00.063436Z","shell.execute_reply":"2022-10-04T06:32:00.087779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_clip_dict = model.state_dict()\nmodel_dict = []\nfor key in sorted(model_clip_dict.keys()):\n    if ('image_encoder' in key) or ('w_' in key): \n        model_dict.append(key)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:32:00.091045Z","iopub.execute_input":"2022-10-04T06:32:00.09204Z","iopub.status.idle":"2022-10-04T06:32:00.102093Z","shell.execute_reply.started":"2022-10-04T06:32:00.092003Z","shell.execute_reply":"2022-10-04T06:32:00.101166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_dict = {k: v for k, v in model_clip_dict.items() if k in model_dict}\nembd_model.load_state_dict(pretrained_dict, strict=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:32:00.103779Z","iopub.execute_input":"2022-10-04T06:32:00.104255Z","iopub.status.idle":"2022-10-04T06:32:00.140625Z","shell.execute_reply.started":"2022-10-04T06:32:00.104219Z","shell.execute_reply":"2022-10-04T06:32:00.139774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embd_model.eval()\nsaved_model = torch.jit.script(embd_model)\nsaved_model.save('saved_model.pt')","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:32:00.141691Z","iopub.execute_input":"2022-10-04T06:32:00.14228Z","iopub.status.idle":"2022-10-04T06:32:01.802797Z","shell.execute_reply.started":"2022-10-04T06:32:00.142244Z","shell.execute_reply":"2022-10-04T06:32:01.801757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from zipfile import ZipFile\nwith ZipFile('submission.zip', 'w') as zip:\n    zip.write('./saved_model.pt', arcname='saved_model.pt')","metadata":{"execution":{"iopub.status.busy":"2022-10-04T06:32:37.564722Z","iopub.execute_input":"2022-10-04T06:32:37.565455Z","iopub.status.idle":"2022-10-04T06:32:39.718196Z","shell.execute_reply.started":"2022-10-04T06:32:37.565416Z","shell.execute_reply":"2022-10-04T06:32:39.717171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize Embeddings","metadata":{}},{"cell_type":"code","source":"# helper method to extract all embedings from a data loader\ndef get_embeddings(model, dl):\n    embs = []\n    ys = []\n    for batch in tqdm(dl):\n        with torch.no_grad():\n            embs.append(model(batch['image']))\n            ys.append(batch['label'])\n    embs = torch.cat(embs)\n    embs = embs / embs.norm(p=2,dim=1)[:,None]\n    ys = torch.cat(ys)\n    return embs,ys\n\n# helper to plot embeddings in 3D\ndef plot_embs(embs, ys, ax):\n    #ax.axis('off')\n    for k in range(10):\n        e = embs[ys==k].cpu()\n        ax.scatter(e[:,0], e[:,1], e[:,2], s=4, alpha=.2)   \n","metadata":{"execution":{"iopub.status.busy":"2022-10-03T01:31:32.56012Z","iopub.execute_input":"2022-10-03T01:31:32.561186Z","iopub.status.idle":"2022-10-03T01:31:32.578531Z","shell.execute_reply.started":"2022-10-03T01:31:32.561122Z","shell.execute_reply":"2022-10-03T01:31:32.577177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MyModel().to(device)\nmodel.load_state_dict(torch.load(\"best.pt\", map_location=device))\ntest_loader = build_loaders(test_df, mode=\"test\")\n\nembs_arcface, ys_arcface  = get_embeddings(model.eval(), test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T01:31:32.58563Z","iopub.execute_input":"2022-10-03T01:31:32.586839Z","iopub.status.idle":"2022-10-03T01:51:42.064405Z","shell.execute_reply.started":"2022-10-03T01:31:32.58673Z","shell.execute_reply":"2022-10-03T01:51:42.062429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_,ax1=plt.subplots(1,1, figsize=(20,10), subplot_kw={'projection':'3d'})\nplot_embs(embs_arcface, ys_arcface, ax1)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T01:51:42.067455Z","iopub.execute_input":"2022-10-03T01:51:42.068464Z","iopub.status.idle":"2022-10-03T01:51:42.510054Z","shell.execute_reply.started":"2022-10-03T01:51:42.068406Z","shell.execute_reply":"2022-10-03T01:51:42.508573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # _,(ax1,ax2)=plt.subplots(1,2, figsize=(20,10), subplot_kw={'projection':'3d'})\n# _,ax1=plt.subplots(1,1, figsize=(20,10), subplot_kw={'projection':'3d'})\n# plot_embs(embs_arcface, ys_arcface, ax1)\n# # plot_embs(embs_softmax, ys_softmax, ax2)\n\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T01:51:42.511427Z","iopub.execute_input":"2022-10-03T01:51:42.517098Z","iopub.status.idle":"2022-10-03T01:51:42.524145Z","shell.execute_reply.started":"2022-10-03T01:51:42.517062Z","shell.execute_reply":"2022-10-03T01:51:42.522611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate with ObjectNet","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-10-03T07:24:07.240205Z","iopub.execute_input":"2022-10-03T07:24:07.240588Z","iopub.status.idle":"2022-10-03T07:24:07.246061Z","shell.execute_reply.started":"2022-10-03T07:24:07.240555Z","shell.execute_reply":"2022-10-03T07:24:07.245051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ONET_DIR_MAP = {\n    int(_dir.split(\"-\",2)[1]):os.path.join(\"/kaggle/input\", _dir, f'split_{_dir.split(\"-\", 2)[1]}') \\\n    for _dir in os.listdir(\"/kaggle/input\") if \"objectnet\" in _dir\n}\nONET_CSV_MAP = {k:pd.read_csv(os.path.join(v, \"onet.csv\")) for k,v in ONET_DIR_MAP.items()}\nfor split_n, split_df in sorted(ONET_CSV_MAP.items(), key=lambda x: x[0]):\n    print(f\"\\n\\n... SPLIT #{split_n:>02} DATAFRAME (LENGTH={len(split_df)}) ...\\n\\n\")\n    display(split_df.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-10-03T07:24:08.8043Z","iopub.execute_input":"2022-10-03T07:24:08.804982Z","iopub.status.idle":"2022-10-03T07:24:08.843561Z","shell.execute_reply.started":"2022-10-03T07:24:08.804946Z","shell.execute_reply":"2022-10-03T07:24:08.842507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ONET_DIR_MAP","metadata":{"execution":{"iopub.status.busy":"2022-10-03T07:24:09.236668Z","iopub.execute_input":"2022-10-03T07:24:09.237017Z","iopub.status.idle":"2022-10-03T07:24:09.243869Z","shell.execute_reply.started":"2022-10-03T07:24:09.236987Z","shell.execute_reply":"2022-10-03T07:24:09.242717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"onet_df = pd.concat(list(ONET_CSV_MAP.values())).reset_index(drop=True)\nonet_df","metadata":{"execution":{"iopub.status.busy":"2022-10-03T07:24:10.862737Z","iopub.execute_input":"2022-10-03T07:24:10.863285Z","iopub.status.idle":"2022-10-03T07:24:10.883347Z","shell.execute_reply.started":"2022-10-03T07:24:10.863249Z","shell.execute_reply":"2022-10-03T07:24:10.882431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"onet_df[\"img_path\"] = \"/kaggle/input/objectnet-\"+\\\n                      onet_df[\"split\"].astype(str)+\"-of-10/\"\\\n                      \"split_\"+onet_df[\"split\"].astype(str)+\\\n                      \"/images/\"+onet_df[\"label\"]+\"/\"+\\\n                      onet_df[\"img_name\"]\n\nprint(\"\\n... FULL LOADED OBJECTNET DATAFRAME ...\\n\")\ndisplay(onet_df)\n\nprint(f\"\\n... OBJECTNET CLASS (N_CLASSES={onet_df.label.nunique()}) DISTRIBUTION ...\\n\")\nfor k,v in onet_df.label.value_counts().items(): print(f\"\\t{k:<20}\\t-->\\tCOUNT={v}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-03T07:24:13.904659Z","iopub.execute_input":"2022-10-03T07:24:13.905017Z","iopub.status.idle":"2022-10-03T07:24:13.948937Z","shell.execute_reply.started":"2022-10-03T07:24:13.904988Z","shell.execute_reply":"2022-10-03T07:24:13.947502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"onet_df[\"img_path\"] = \"/kaggle/input/objectnet-\"+\\\n                      onet_df[\"split\"].astype(str)+\"-of-10/\"\\\n                      \"split_\"+onet_df[\"split\"].astype(str)+\\\n                      \"/images/\"+onet_df[\"label\"]+\"/\"+\\\n                      onet_df[\"img_name\"]\n\nprint(\"\\n... FULL LOADED OBJECTNET DATAFRAME ...\\n\")\ndisplay(onet_df)\n\nprint(f\"\\n... OBJECTNET CLASS (N_CLASSES={onet_df.label.nunique()}) DISTRIBUTION ...\\n\")\nfor k,v in onet_df.label.value_counts().items(): print(f\"\\t{k:<20}\\t-->\\tCOUNT={v}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-03T07:24:16.28899Z","iopub.execute_input":"2022-10-03T07:24:16.28958Z","iopub.status.idle":"2022-10-03T07:24:16.323695Z","shell.execute_reply.started":"2022-10-03T07:24:16.289546Z","shell.execute_reply":"2022-10-03T07:24:16.322636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"onet_df.to_csv(\"full_onet.csv\", index=False)\nprint(os.listdir(\"/kaggle/working\"))","metadata":{"execution":{"iopub.status.busy":"2022-10-03T07:24:17.332123Z","iopub.execute_input":"2022-10-03T07:24:17.332506Z","iopub.status.idle":"2022-10-03T07:24:17.357552Z","shell.execute_reply.started":"2022-10-03T07:24:17.332475Z","shell.execute_reply":"2022-10-03T07:24:17.356588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_onet = onet_df[['img_path', 'onet_str_label', 'int_label']]","metadata":{"execution":{"iopub.status.busy":"2022-10-03T07:24:18.977966Z","iopub.execute_input":"2022-10-03T07:24:18.978956Z","iopub.status.idle":"2022-10-03T07:24:18.986545Z","shell.execute_reply.started":"2022-10-03T07:24:18.97891Z","shell.execute_reply":"2022-10-03T07:24:18.985469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_onet.columns = ['img_path','class','encoded_label']","metadata":{"execution":{"iopub.status.busy":"2022-10-03T07:24:20.038838Z","iopub.execute_input":"2022-10-03T07:24:20.0392Z","iopub.status.idle":"2022-10-03T07:24:20.046594Z","shell.execute_reply.started":"2022-10-03T07:24:20.03917Z","shell.execute_reply":"2022-10-03T07:24:20.043395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MyModel().to(device)\nmodel.load_state_dict(torch.load(\"best.pt\", map_location=device))\nonet_loader = build_loaders(df_onet, mode=\"test\")\n\n# embs_arcface, ys_arcface  = get_embeddings(embd_model.eval(), onet_loader)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T07:24:21.654166Z","iopub.execute_input":"2022-10-03T07:24:21.654546Z","iopub.status.idle":"2022-10-03T07:24:29.252137Z","shell.execute_reply.started":"2022-10-03T07:24:21.654515Z","shell.execute_reply":"2022-10-03T07:24:29.251073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_embeds(model):\n    embeds = []\n    labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(onet_loader):\n            out = model(batch['image'].to('cuda'))\n            embeds.append(out)\n            labels.append(batch['label'])\n\n    embeds = torch.cat(embeds)\n    labels = torch.cat(labels)\n    return (embeds, labels)\n\ndef normalize(a, eps=1e-8):\n    a_n = a.norm(dim=1)[:, None]\n    a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n    return a_norm\n\n# The following function first computes the cosine similarity of each of the embeddings to all other ones and then returns the top-k most similar embeddings for each entery. \n# So, it does a simple KNN and returns the indices of the closest embeddings for each of the enteries in the embeds input tensor.\ndef k_nearest_neighbors(embeds, k=5):\n    normalized = normalize(embeds)\n    preds = normalized @ normalized.T\n    vals, indices = preds.sort(dim=1, descending=True)\n    k += 1\n    return indices[:, 1:k].long()\n\n# I wrote this to let you analyze a model with one function call\ndef evaluate(model, k=5):\n    embeds, labels = get_embeds(model)\n    preds = k_nearest_neighbors(embeds, k=k)\n    accs = (labels[preds] == labels.view(-1, 1)).float().mean(dim=1)\n    return accs.mean()\n\n# The following function first computes the cosine similarity of each of the embeddings to all other ones and then returns the top-k most similar embeddings for each entery. \n# So, it does a simple KNN and returns the indices of the closest embeddings for each of the enteries in the embeds input tensor.\ndef k_nearest_neighbors(embeds, k=5):\n    normalized = normalize(embeds)\n    preds = normalized @ normalized.T\n    vals, indices = preds.sort(dim=1, descending=True)\n    k += 1\n    return indices[:, 1:k].long()\n\n# I wrote this to let you analyze a model with one function call\ndef evaluate(model, k=5):\n    embeds, labels = get_embeds(model)\n    preds = k_nearest_neighbors(embeds, k=k)\n    accs = (labels[preds] == labels.view(-1, 1)).float().mean(dim=1)\n    return accs.mean()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T07:24:29.253964Z","iopub.execute_input":"2022-10-03T07:24:29.254537Z","iopub.status.idle":"2022-10-03T07:24:29.269951Z","shell.execute_reply.started":"2022-10-03T07:24:29.254501Z","shell.execute_reply":"2022-10-03T07:24:29.268944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(model)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T07:24:29.27133Z","iopub.execute_input":"2022-10-03T07:24:29.271941Z","iopub.status.idle":"2022-10-03T07:39:28.638322Z","shell.execute_reply.started":"2022-10-03T07:24:29.2719Z","shell.execute_reply":"2022-10-03T07:39:28.637006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#0.23 -> 0.44","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}